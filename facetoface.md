# facetoface

## java基础

### java语言的优缺点

- 面向对象的 

- 平台无关的 

- 提供了很多类库：比如对多线程的支持  

- 提供了对web的支持 

- 具有较好的安全性和健壮性 

- 祛除了c++语言总难以理解点的特性 比如，头文件 指针 等等     

  缺点：~~开发效率低~~

### finalize

- 首先，大致描述一下finalize流程：当对象变成(GC Roots)不可达时，GC会判断该对象是否覆盖了finalize方法，若未覆盖，则直接将其回收。否则，若对象未执行过finalize方法，将其放入F-Queue队列，由一低优先级线程执行该队列中对象的finalize方法。执行finalize方法完毕后，GC会再次判断该对象是否可达，若不可达，则进行回收，否则，对象“复活”

### BIO/NIO/AIO

- BIO（同步阻塞）客户端有连接请求时服务器就需要启动一个线程进行处理-可以通过线程池机制进行改善
- NIO:客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有IO请求时才启动一个线程进行处理（多而短的连接）EPOLL
- AIO:客户端的IO请求都是由操作系统先完成了再通知服务器用其启动线程进行处理(多而长的连接)   UNIX网络编程中的事件驱动I/O（AIO
- BIO/NIO 区别：BIO面向流，NIO面向缓冲区 BIO的各种流是阻塞的，NIO是非阻塞模式Java NIO的选择允许一个单独的线程来监视多个输入通道，可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择”通道：这些通道里已经有可以处理的输入或选择已准备写入的通道。这种选择机制，使得一个单独的线程很容易来管理多个通道ChannelsBuffersSelectors

### 反射

- 定义：Java反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为Java语言的反射机制。
- 三种方式：getClass   .class    class.fromname
- 使用场景：使用反射虽然会很大程度上提高代码灵活性，但不能滥用反射，因为 通过反射创建对象时性能稍微低一些。实际上，只有当程序需要动态创建某个类的对象时才会考虑使用反射。通常在开发通用性比较广的框架，基础平台时可能大量使用反射。因为在很多java框架中都需要根据配置信息创建Java对象，从配置文件读取的只是某个类的字符串类名，程序员需要根据字符串来创建对应的实例，就必须使用java.
- class.forname和classloader区别

	-     forName("")得到的class是已经初始化完成的
  -     loadClass("")得到的class是还没有连接的
        一般情况下，这两个方法效果一样，都能装载Class。
        但如果程序依赖于Class是否被初始化，就必须用Class.forName(name)了。

### 序列化/反序列化

- 序列化就是将java对象转化为字节的过程  反序列化：客户端从文件中或网络上获得序列化后的对象字节流后，根据字节流中所保存的对象状态及描述信息，通过反序列化重建对象
- 实现方式：ObjectOutputStream: 表示输出对象流它的writeObject(Object obj)方法可以对参数指定的obj对象进行序列化  ObjectInputStream:表示对象输入流它的readObject()方法源输入流中读取字节序列，再把它们反序列化成为一个对象
- serialVersionUID是序列化和反序列化之间彼此认识的唯一信物，如果没有指定Person类的serialVersionUID的，那么java编译器会自动给这个class进行一个摘要算法
- XML   JSON  Thrift Hessian

### String  StringBuffer  StringBuilder

- String的值是不可变的，这就导致每次对String的操作都会生成新的String对象，不仅效率低下，而且浪费大量优先的内存空间	 
- StringBuffer是可变类，和线程安全的字符串操作类，任何对它指向的字符串的操作都不会产生新的对象。每个StringBuffer对象都有一定的缓冲区容量，当字符串大小没有超过容量时，不会分配新的容量，当字符串大小超过容量时，会自动增加容量 	
- StringBuilder可变类，速度更快

### Object类

- wait/notify
- finalize
- clone

### java集合

#### ArrayList 和 LinkedList区别

- arraylist底层是数组结构 而linkedlist底层是双向链表
- LinkedList 

  - getFirst(),element(),peek(),peekFirst()：这四个获取头结点方法的区别在于对链表为空时的处理，是抛出异常还是返回null
  - remove() ,removeFirst(),pop() poll(): 删除头节点  为空时 poll返回null  remove pop 抛异常
  - **扩展LRU算法**

- ArrayList为什么默认只扩容1.5倍：如果扩容两倍的话 之前释放的内存将永远不可复用（一直都小于要扩容的量）如果是1.5倍的话 可以复用之前回收的空间

#### LIST  SET  MAP的区别

- List:允许重复,可以插入多个null,是一个有序的容器，保持了每个元素的插入顺序，输出的顺序就是插入顺序

- SET：不允许重复，只允许一个null，无序

- MAP：

  1.Map不是collection的子接口或者实现类。Map是一个接口。
  2.Map 的 每个 Entry 都持有两个对象，也就是一个键一个值，Map 可能会持有相同的值对象但键对象必须是唯一的。
3. TreeMap 也通过 Comparator  或者 Comparable 维护了一个排序顺序。
4. Map 里你可以拥有随意个 null 值但最多只能有一个 null 键。

#### HashMap

- put流程

  - 流程判断

     1. 如果当前Node数组(tab)为空，则直接创建(通过resize()创建)，并将当前创建后的长度设置给n

     2. 如果要存放对象所在位置的Node节点为空，则直接将对象存放位置创建新Node，并将值直接存入

     3. 存放的Node数组不为空，且存放的下标节点Node不为空（该Node节点为链表的首节点）
           * 1. 比较链表的首节点存放的对象和当前存放对象是否为同一个对象，如果是则直接覆盖并将原来的值返回

           * 2. 如果不是分两种情况

             1. 存储处节点为红黑树node结构，调用方法putTreeVal()直接将数据插入(中间也会判断树中是否存在当前节点)

             2. 不是红黑树，则表示为链表，则进行遍历

             - 如果存入的链表下一个位置为空，则先将值直接存入，存入后检查当前存入位置是否已经大于链表的第8个位置 
               - 如果大于,调用treeifyBin方法判断是扩容 还是 需要将该链表转红黑树（大于8且总数据量大于64则转红黑色，否则对数组进行扩容）
               - 当前存入位置链表长度没有大于8，则存入成功，终端循环操作。

             - 如果存入链表的下一个位置有值，且该值和存入对象“一样”，则直接覆盖，并将原来的值返回

             3. 上面两种情况执行完成后，判断返回的原对象是否为空，如果不为空，则将原对象的原始value返回

           上面123三种情况下，如果没有覆盖原值，则表示新增存入数据，存储数据完成后，size+1,然后判断当前数据量是否大于阈值，

           如果大于阈值，则进行扩容。

- resize流程

  - 需要扩容的时机：
    1. 第一次插入 数组为空时 进行扩容 
    2. 插入链表 这时链表长度大于等于8并且多有节点小于64 则进行扩容 
    3. 插入之后size >threshold 则进行扩容

- 长度为什么是2次幂

  - 1：我们首先可能会想到采用%取余的操作来实现。但是，重点来了：“取余(%)操作中如果除数是2的幂次则等价于与其除数减一的与(&)操作（也就是说 hash%length==hash&(length-1)的前提是 length 是2的 n 次方；）。” 并且 采用二进制位操作 &，相对于%能够提高运算效率，这就解释了 HashMap 的长度为什么是2的幂次方。
  - 2：只有当对应位置的数据都为1时，运算结果也为1，当HashMap的容量是2的n次幂时，(n-1)的2进制也就是1111111***111这样形式的，这样与添加元素的hash值进行位运算时，能够充分的散列，使得添加的元素均匀分布在HashMap的每个位置上，减少hash碰撞

- 为什么不直接使用红黑树直接代替链表

  - 树的节点占的空间是普通节点的两倍，在节点足够多的时候才会使用树形数据结构，如果节点变少了还是会变回普通节点。总的来说就是节点太少的时候没必要转换、不仅转换后的数据结构占空间而且转换也需要花费时间。
  据统计链表中的节点数是8的概率已经接近千分之一且此时链表的性能已经很差。所以在这种比较罕见的和极端的情况下才会把链表转变为红黑树。
  就是说大部分情况下HashMap还是使用链表，如果理想的均匀分布节点数不到8就已经自动扩容了

#### concurrentHashMap为什么放弃分段锁

- 加入多个分段锁浪费内存空间。
生产环境中， map 在放入时竞争同一个锁的概率非常小，分段锁反而会造成更新等操作的长时间等待。
为了提高 GC 的效率
- 锁的粒度
首先锁的粒度并没有变粗，甚至变得更细了。每当扩容一次，ConcurrentHashMap的并发度就扩大一倍。
Hash冲突
JDK1.7中，ConcurrentHashMap从过二次hash的方式（Segment -> HashEntry）能够快速的找到查找的元素。在1.8中通过链表加红黑树的形式弥补了put、get时的性能差距。
扩容
JDK1.8中，在ConcurrentHashmap进行扩容时，其他线程可以通过检测数组中的节点决定是否对这条链表（红黑树）进行扩容，减小了扩容的粒度，提高了扩容的效率。
下面是我对面试中的那个问题的一下看法：

##### 为什么是synchronized，而不是ReentranLock

1. 减少内存开销
假设使用可重入锁来获得同步支持，那么每个节点都需要通过继承AQS来获得同步支持。但并不是每个节点都需要获得同步支持的，只有链表的头节点（红黑树的根节点）需要同步，这无疑带来了巨大内存浪费。
2. 获得JVM的支持
可重入锁毕竟是API这个级别的，后续的性能优化空间很小。
synchronized则是JVM直接支持的，JVM能够在运行时作出相应的优化措施：锁粗化、锁消除、锁自旋等等。这就使得synchronized能够随着JDK版本的升级而不改动代码的前提下获得性能上的提升。

- initTable:初始化 1：SIZECTL是否小于零 小于零则说明有其他线程在初始化table了  不小于0则使用cas 将SIZECTL -1  成功了 则当前线程去初始化table
- put流程

	-  * 当添加一对键值对的时候，首先会去判断保存这些键值对的数组是不是初始化了，
     * 如果没有的话就初始化数组
     *  然后通过计算hash值来确定放在数组的哪个位置
     * 如果这个位置为空则直接添加，如果不为空的话，则取出这个节点来
     * 如果取出来的节点的hash值是MOVED(-1)的话，则表示当前正在对这个数组进行扩容，复制到新的数组，则当前线程也去帮助复制
     * 最后一种情况就是，如果这个节点，不为空，也不在扩容，则通过synchronized来加锁，进行添加操作
     *    然后判断当前取出的节点位置存放的是链表还是树
     *    如果是链表的话，则遍历整个链表，直到取出来的节点的key来个要放的key进行比较，如果key相等，并且key的hash值也相等的话，
     *          则说明是同一个key，则覆盖掉value，否则的话则添加到链表的末尾
     *    如果是树的话，则调用putTreeVal方法把这个元素添加到树中去
     *  最后在添加完成之后，会判断在该节点处共有多少个节点（注意是

## jvm

### 内存结构

- 堆

	- 对大多数应用来说，Java堆是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例。
Java堆是垃圾收集器管理的主要区域。从内存回收的角度来看，由于现在收集器基本都采用分代收集算法，所以Java堆中还可以分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。从内存分配的角度来看，线程共享的Java堆中可能划分出多个线程私有的分配缓冲区（Thread Local Allocation Buffer，TLAB)。
Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。
enden和old 1:3   eden survivor 3:1:1

- 虚拟机栈（私有）

	- Java虚拟机栈也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧（StackFrame)用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用到执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。

- 本地方法栈（私有）

	- 本地方法栈（Native Method Stack)与虚拟机栈所发挥的作用是非常相似的，它们之间的区别不过是虚拟机栈为虚拟机执行Java方法（字节码)服务，而本地方法栈为虚拟机使用到的Native方法服务

- 程序计数器

	- 程序计数器（Program Counter Register)是一块很小的内存区域，它可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型中，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。
由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式实现的，在任何一个确定的时刻，一个处理器都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类内存区域为线程私有的内存。
如果线程正在执行的是Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Native方法， 这个计数器的值为空。此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域

- 方法区

	- 方法区（Method Area)与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆)，目的应该是与Java堆区分开来。
	
	  **<u>OOM（jstack）/（jmap -histo 2333| head -20  占内存最大的前二十个对象）</u>**
	
- 堆内存溢出

  - java堆用于存储对象实例，只要不断增加对象，并且保证GC Roots到对象之间有可达路径来避免垃圾回收机制清除这些对象，那么在对象数量达到最大堆的容量限制后就会产生OOM异常。
  - 要解决这个区域的异常，一般的手段是通过内存映像分析工具对dump出来的堆转储快照进行分析，重点是确认内存中的对象是否是必要的，也就是要判断是出现来内存泄露还是内存溢出。前者的话要进一步通过工具查看泄露对象到GC Roots的引用链；后者的话可以调大虚拟机的堆参数（-Xms和-Xmx)，或者从代码上检查某些对象生命周期过长等。  用jstack  查看栈帧   jmap查看对象 但是jmap 在内存比较大，jamp会对进程产生大的影响，甚至卡顿 解决办法 1.设置参数-XX：+HeapDUmpOnOutOfMemeryError  转存oom 错误到硬盘文件  2.多台服务器备用 3.测试环境压测

- 栈内存溢出
- 方法区内存溢出

  - 注意Java8下运行时常量池在堆中，所以运行时常量池过大会体现为OOM：heap；
  而在此以前是放在永久代中，体现为OOM：PermGen space。
  方法区还存放Class的相关信息，运行时产生大量的类也会导致方法区（Java8中放在直接内存中)溢出。

- 直接内存溢出

### 垃圾回收

![image-20210804181645390](https://raw.githubusercontent.com/lxcsjk/oss/master/uPic/w1OA8ourVqgCmyN-20210817003455802.jpg)

- 垃圾回收算法

	- 标记清除算法

		- 标记清除算法就是分为“标记”和“清除”两个阶段。标记出所有需要回收的对象，标记结束后统一回收。这个套路很简单，也存在不足，后续的算法都是根据这个基础来加以改进的。
其实它就是把已死亡的对象标记为空闲内存，然后记录在一个空闲列表中，当我们需要new一个对象时，内存管理模块会从空闲列表中寻找空闲的内存来分给新的对象。
不足的方面就是标记和清除的效率比较低下。且这种做法会让内存中的碎片非常多。这个导致了如果我们需要使用到较大的内存块时，无法分配到足够的连续内存

	- 复制算法

		- 为了解决效率问题，复制算法就出现了。它将可用内存按容量划分成两等分，每次只使用其中的一块。和survivor一样也是用from和to两个指针这样的玩法。fromPlace存满了，就把存活的对象copy到另一块toPlace上，然后交换指针的内容。这样就解决了碎片的问题。
这个算法的代价就是把内存缩水了，这样堆内存的使用效率就会变得十分低下了

	- 标记整理算法

		- 复制算法在对象存活率高的时候会有一定的效率问题，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉边界以外的内存

	- 分代收集算法

		- 这种算法并没有什么新的思想，只是根据对象存活周期的不同将内存划分为几块。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或者“标记-整理”算法来进行回收。

- fullGC/MinorGC

	- minorGC

		- 当Eden空间满了之后，会触发一个叫做Minor GC（就是一个发生在年轻代的GC）的操作，存活下来的对象移动到Survivor0区。Survivor0区满后触发 Minor GC，就会将存活对象移动到Survivor1区，此时还会把from和to两个指针交换

	- fullGC

		- 程序执行了System.gc() //建议jvm执行fullgc，并不一定会执行
执行了jmap -histo:live pid命令 //这个会立即触发fullgc
在执行minor gc的时候进行的一系列检查

- 垃圾回收器

  - Serial 收集器（新生代采用复制算法，老年代采用标记-整理算法）

  	- Serial（串行）收集器收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ "Stop The World" ），直到它收集结束。

  - ParNew 收集器（新生代采用复制算法，老年代采用标记-整理算法。）

  	- ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。

  - CMS 收集器（标记【写屏障+三色标记：错标】-清除）

    - 初始标记： 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快 ；

    - 并发标记： 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。

    - 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短

    - 并发清除： 开启用户线程，同时 GC 线程开始对未标记的区域做清扫。

      从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面三个明显的缺点：

      - 对 CPU 资源敏感；
      - 无法处理浮动垃圾；
      - 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。

  - G1收集器（逻辑分区  物理不分区 标记-整理算法，不产生内存碎片。）三色标记+STAB+写屏障

    - 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。
    - 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。
    - 空间整合：与 CMS 的“标记--清理”算法不同，G1 从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。
    - 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。
      	- G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来)。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。将region复制到另一个region中

  - ZGC（颜色指针+读屏障）

  	- 逻辑上一次ZGC分为Mark（标记）、Relocate（迁移）、Remap（重映射）三个阶段
  	- Mark: 所有活的对象都被记录在对应Page的Livemap（活对象表，bitmap实现）中，以及对象的Reference（引用）都改成已标记（Marked0或Marked1）状态
  	- Relocate: 根据页面中活对象占用的大小选出的一组Page，将其中中的活对象都复制到新的Page, 并在额外的forward table（转移表）中记录对象原地址和新地址对应关系
  	- Remap: 所有Relocated的活对象的引用都重新指向了新的正确的地址

实现上，由于想要将所有引用都修正过来需要跟Mark阶段一样遍历整个对象图，所以这次的Remap会与下一次的Remark阶段合并。


  - jdk 1.8 ：PS+ParallelOld     垃圾回收器的发展路线是随着内存越来越大的过程而演变的从分代算法演变到不分代算法  serial支持几十兆 parallel 几个G   CMS 支持几十个G  G1 支持上百G 内存 ZGC  4T

![image-20210804181740999](https://raw.githubusercontent.com/lxcsjk/oss/master/uPic/qbWkmU6wzuvpYdT.jpg)

### 对象

- 对象是够可活

  - 引用计数器

  	- 很多教科书判断对象是否存活的算法是这样的：给对象添加一个引用计数器，每当有一个地方引用它时，计数器就加1；当引用失效时，计数器值就减1；任何时刻计算器为0的对象就是不可能再被使用的。
  主流的Java虚拟机中没有选用计数算法来管理内存，最主要的原因是它很难就解决对象之间相互循环引用的问题。

  - 可达性分析

    - 主流的商用程序语言的主流实现中，都是称通过可达性分析（Reachability Analysis)来判定对象是否存活的。这个算法的基本思路就是通过一系列的称为GC Roots 的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain)，当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可达的。下图章，对象object5、object6、object7虽然互相有关联，但是它们到GC Roots时不可达的，所以它们将会被判定为可回收的对象。
      在Java中，可作为GC Roots的对象包括：

      虚拟机栈中引用的对象

      方法区中类静态属性引用的对象 

      方法区中常量引用的对象

      本地方法栈中JNI（一般说的Native方法)引用的对象

      三色标记：
      初始时，所有对象都在 【白色集合】中；
      将GC Roots 直接引用到的对象 挪到 【灰色集合】中；
      从灰色集合中获取对象：
      3.1. 将本对象 引用到的 其他对象 全部挪到 【灰色集合】中；
      3.2. 将本对象 挪到 【黑色集合】里面。
      重复步骤3，直至【灰色集合】为空时结束。
      结束后，仍在【白色集合】的对象即为GC Roots 不可达，可以进行回收。(三色标记的缺点：缺标  )

 - finalize
	
		- 即使在可达性分析中不可达的对象，也并非是非死不可。要真正宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件就是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法，或者finalize()已经被虚拟机调用过，虚拟机将这两种情况视为没有必要执行。
	如果这个对象被判为有必要执行finalize()方法，那么这个对象将会放置在一个叫做F-Queue队列之中，并在稍后由一个虚拟机自动建立的、低优先级的Finalizer线程去执行它。这里所谓的执行是指虚拟机会触发这个方法，但并不承诺会等待它运行结束，这样做的原因是，如果一个对象在finalize()方法中执行缓慢，或者发生了死循环，将很可能会导致F-Queue队列中其他对象永久处于等待，甚至导致整个内存回收系统崩溃。finalize()方法是对象逃脱死亡命运的最后一次机会，稍后GC将会对F-Queue中的对象进行第二次小规模的标记，\如果对象要在finalize()中拯救自己，只要重新与引用链上的任何一个对象建立联系即可，比如把自己this复制给某个类变量或对象的成员变量，那在第二次标记时它将被移出即将回收的集合；如果对象这时候还没有逃脱，那基本上它就真的被回收了。任何一个对象的finalize()方法都只会被系统调用一次，如果对象面临下一次回收，它的finalize()方法不会被再次执行。

	- 回收方法区
	
		- 判定一个类是否是无用的类的条件比较苛刻，需要同时满足以下三个条件：

			1)该类的所有实例都已经被回收
			2)加载该类的类加载器已经被回收
			3)该类对应的Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。

- 对象的创建

  - 虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那就执行类加载过程。
    在类加载检查通过后，虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来。假设Java堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪动一块与对象大小相等的距离，这种分配方式称为指针碰撞。如果Java堆中的内存不是规整的，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为空闲列表（Free List)。选择哪种分配方式由Java堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。
    除如何划分可用空间之外，还有另外一个需要考虑的问题是对象创建在虚拟机中是非常频繁的行为，即使是仅仅修改一个指针所指向的位置，在并发情况下也不是线程安全的，可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。解决这个问题有两个方案，一种是对分配内存空间的动作进行同步处理，另一种是把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲TLAB。哪个线程分配内存，就在哪个线程的TLAB上分配，只有TLAB用完并分配新的TLAB时，才需要同步锁定。
    内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值。接下来，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何找到类的元数据等信息。这些信息存放在对象的对象头之中。上述工作完成后，从虚拟机的视角来看，一个新的对象已经产生，但从Java程序的视角来看，构造方法还没有执行，字段都还为0。所以执行new指令之后会接着执行构造方法等，这样一个对象才算真正产生出来。

  - Object  o = new Object();  这条指令的执行

    1：new#2--在堆中创建一块内存，并为成员变量设置默认值   

    2：invokerspecial -- 为变量赋值  

    3：astore 将栈帧指向这块内存（2 3 可能会发生指令重排序）

    ![image-20210730103700692](https://raw.githubusercontent.com/lxcsjk/oss/master/uPic/image-20210730103700692.png)

- 对象的内存布局

	- 在HotSpot虚拟机中，对象在内存中存储的布局可以分为3个区域：对象头（Header)、实例数据（Instance Data)和对齐填充（Padding)。
对象头包括两部分信息，第一部分用于存储对象自身的运行时数据，如哈希码、GC分代年龄、锁状态标志等。对象头的另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。
实例数据是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容。无论是从父类继承的，还是在子类中定义的，都需要记录下来。相同宽度的字段总是被分配到一起，在这个前提下，在父类中定义的变量会出现在子类之前。
对齐填充并不是必然存在的，它仅仅起着占位符的作用，HotSpot虚拟机的自动内存管理系统要求对象起始地址必须是8字节的整数倍，即对象大小必须是8字节的整数倍，而对象头正好是8字节的整数倍。因此，当对象实例数据部分没有对齐时，就需要对齐填充来补全

- 对象的访问定位

	- Java程序需要通过栈上的Reference数据来操作堆上的具体对象。由于Reference类型在Java虚拟机规范中只规定了一个指向对象的引用，并没有定义这个引用应该通过何种方式来定位、访问堆中对象的具体位置，所以对象访问方式也是取决于虚拟机实现而定的。目标主流的方式有使用句柄和直接指针两种。
如果使用句柄访问的话，那么Java堆中将会划分出一块内存来作为句柄池，Reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据和类型数据各自的具体地址信息。
如果使用直接指针访问，那么Java堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而Reference中存储的直接就是对象地址。
使用句柄来访问的最大好处就是Reference中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而Reference本身不需要修改。
使用直接指针访问方式的最大好处就是速度更快，它节省了一次指针定位的时间开销。

	- 使用句柄来访问的最大好处就是Reference中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而Reference本身不需要修改

- 初始化顺序
	1. 父类静态成员和静态初始化快，按在代码中出现的顺序依次执行。
	2. 子类静态成员和静态初始化块，按在代码中出现的顺序依次执行。
	3. 父类的实例成员和实例初始化块，按在代码中出现的顺序依次执行。
	4. 执行父类的构造方法。
	5. 子类实例成员和实例初始化块，按在代码中出现的顺序依次执行。
	6. 执行子类的构造方法。

### 类

- 类的加载机制

	- 类的整个生命周期包括：加载、验证、准备、解析、初始化、使用和卸载。

- 类的主动引用和被动引用

	- new对象
	引用静态变量（static非final)和静态方法
	反射
	main方法所在类
	当前类的所有父类
	- 被动引用（不会发生类的初始化)

访问一个类的静态变量，只有真正声明这个静态变量的类才会被初始化

通过数组定义类引用

引用常量（存在方法区的运行时常量

- 双亲委派

	- 启动类加载器（bootstrap classloader)
	- 扩展类加载器（Extension ClassLoader)
	- 应用程序类加载器（Application ClassLoader)
	- 双亲委派模型的工作过程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求，子加载器才会尝试自己去加载。
	使用双亲委派模型来组织类加载器之间的关系，有一个显而易见的好处就是Java类随它的类加载器一起具备了一种带有优先级的层次关系。
	- 打破双亲委派:自己写一个类加载器
	  - 2：重写loadclass方法
	  - 3：重写findclass方法
- tomcat双亲委派
	 Tomcat的类加载机制是违反了双亲委托原则的，对于一些未加载的非基础类(Object,String等)，各个web应用自己的类加载器(WebAppClassLoader)会优先加载，加载不到时再交给commonClassLoader走双亲委托。 

### 内存分配原则

- 对象优先在Eden分配
大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC。如果GC期间虚拟机发现已有的对象全部无法放入Survivor空间，会通过分配担保机制提前转移至老年代中。
- 大对象直接进入老年代
所谓的大对象是指，需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串以及数组。经常出现大对象容易导致内存还有不少空间就提前触发垃圾收集以获取足够的连续空间来安置它们。
- 长期存活的对象将进入老年代
虚拟机为每个对象定义一个对象年龄（Age)计数器。如果对象在Eden出生并经过第一次Minor GC后仍然存活，并且能够被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设为1.对象在Survivor区中每熬过一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁)，就将会被晋升到老年代。
- 动态对象年龄判定
虚拟机并不是永远地要求对象的年龄必须达到MaxTenuringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代。
- 空间分配担保
在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管这次Minor GC是有风险的；如果小于，或者HandlePromotionFailure设置不允许冒险，那此时也要改为进行一次Full GC。
冒险是指当出现大量对象在Minor GC后仍然存活的情况，就需要老年代进行分配担保，把Survivor区无法容纳的对象直接进入老年代。老年代要进行这样的担保，前提是老年代本身还有容纳这些对象的剩余空间，一共会有多少对象会活下来在实际完成内存回收之前是无法明确知道的，所以只好取之间每一次回收晋升到老年代对象容量的平均大小值作为经验值，与老年代的剩余空间进行比较，决定是否进行Full GC来让老年代腾出更多空间。
取平均值进行比较其实仍然是一种动态概率的手段，依然存在担保失败的情况。如果出现了HandlePromotionFailure失败，那就只好在失败后重新发起一次Full GC。

### JMM

- 缓存一致性：每条线程都有自己的工作内存，里面保存了被该线程使用到的变量的主内存副本拷贝，线程对变量的所有操作都必须在工作内存中进行，不能直接读写主内存中的变量，相似的，Java虚拟机也定义了一套内存访问协议来保证内存一致性；
- 指令重排序：对应于处理器乱序执行，Java虚拟机的即时编译器中也有着类似的优化，同样只保证最终结果的一致性。
- <img src="https://raw.githubusercontent.com/lxcsjk/oss/master/uPic/image-20210809121947906.png" alt="image-20210809121947906" style="zoom: 25%;" />
- 现代计算机内存模型

	- 主内存对应的是硬件的物理内存，工作内存对应的是寄存器和高速缓存。
	- 现代的处理器使用写缓冲区临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟
	- CPU三级缓存：CPU cache 分为三级:L1 L2 L3,却靠近CPU缓存越小，所以L1很小但很快，并且紧靠近他的CPU，L2大一些并且也只能有一个CPU共用

## 多线程

### CountDownLatch /CyclicBarrier

- CountDownLatch	

  - 1：减数计数方式    
  - 2：减数为0时释放所有等待线程（一个线程等待其他线程结束后再执行）
  - 3：调动countDown方法 计数减一，调用await（）方法只进行阻塞   
  - 4：不能重复使用

- CyclicBarrier

	- 1：加数计数   
	- 2：计数达到指定的值之后释放所有等待线程  
	- 3：计数达到指定值之后重置为0；
	- 4：调用await（）方法  加一  加一之后不等于指定的值  则阻塞  
	- 5：可重复利用

### Fork/Jion

- 在有必要的情况下将一个大任务划分成一个一个的小任务，然后再将各个任务的结果join起来。
- ForkJoinTask：我们要使用ForkJoin框架，必须首先创建一个ForkJoin任务。它提供在任务中执行fork()和join()操作的机制，通常情况下我们不需要直接继承- ForkJoinTask类，而只需要继承它的子类，Fork/Join框架提供了以下两个子类：
oRecursiveAction：用于没有返回结果的任务。
oRecursiveTask ：用于有返回结果的任务。
ForkJoinPool ：ForkJoinTask需要通过ForkJoinPool来执行，任务分割出的子任务会添加到当前工作线程所维护的双端队列中，进入队列的头部。当一个工作线程的队列里暂时没有任务时，它会随机从其他工作线程的队列的尾部获取一个任务。
- 与线程池的区别：某个子任务要等待另一个子任务完成  forkJon会主动找其他尚未运行的子问题来执行

### volatile

- volatile：会保证数据在读操作之前，上一次写操作必须生效，即写回。
1）修改volatile变量时会强制将修改后的值刷新到主内存中。
2）修改volatile变量后会导致其他线程工作内存中对应的变量值失效。因此，再读取该变量值的时候就需要重新从读取主内存中的值。相较于synchronized是一种较为轻量级的同步策略，但是volatile不具备互斥性；不能保证修改变量时的原子性。
- 汇编上的实现

volatile修饰的共享变量在转换为汇编语言后，会出现Lock前缀指令，该指令在多核处理器下引发了两件事：

1、将当前处理器缓存行（CPU cache中可以分配的最小存储单位）的数据写回到系统内存。
2、这个写回内存的操作使得其他CPU里缓存了该内存地址的数据无效。 （当前CPU该变量的缓存回写；其他CPU该变量的缓存失效）
内存语义

volatile写的内存语义：

当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值刷新到主内存 volatile读的内存语义：
当读一个volatile变量时，JMM会把线程对应的本地内存置为无效，线程接下来将从主内存读取共享变量
一个volatile变量的单个读写操作，与一个普通变量的读写操作使用同一个锁来同步，它们的执行效果相同。锁的happens-before规则保证释放锁和获取锁的两个线程之间的内存可见性，这也意味着对一个volatile变量的读操作，总是能看到任意线程对该变量最后的写入。

对于volatile变量本身的单个读写操作具有原子性，但是与锁不同的是，多个对于volatile变量的复合操作不具有原子性。而锁的语义保证了临界区代码的执行具有原子性。

JAVA1.5后，JSR-133增强了volatile的内存语义，严格限制编译器和CPU对于volatile变量与普通变量的重排序，从而确保volatile变量的写-读操作可以实现线程之间的通信，提供了一种比锁更轻量级的线程通信机制。从内存语义的角度而言，volatile的写-读与锁的释放-获取有相同的内存效果：写操作=锁的释放；读操作=锁的获取。
- 保证可见性和指令重排序

  观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令”

  lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能：

  1）它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成；

  2）它会强制将对缓存的修改操作立即写入主存；

  3）如果是写操作，它会导致其他CPU中对应的缓存行无效。

- 可见性：根据缓存一致性
  实现的原理一般都是基于CPU的MESI协议（缓存一致性协议），其中E表示独占Exclusive，S表示Shared，M表示Modify，I表示Invalid，如果一个核心修改了数据，那么这个核心的数据状态就会更新成M，同时其他核心上的数据状态更新成I，这个是通过CPU多核之间的嗅探机制实现的。
  嗅探
  每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。

- 总线风暴
  由于Volatile的MESI缓存一致性协议，需要不断的从主内存嗅探和cas不断循环，无效交互会导致总线带宽达到峰值。
  所以不要大量使用Volatile，至于什么时候去使用Volatile什么时候使用锁，根据场景区分。
  我们再来聊一下指令重排序的问题

- 指令重排序：这个涉及到内存屏障(Memory Barrier)，内存屏障有两个能力：

  a、就像一套栅栏分割前后的代码，阻止栅栏前后的没有数据依赖性的代码进行指令重排序，保证程序在一定程度上的有序性。
  b、强制把写缓冲区/高速缓存中的脏数据等写回主内存，让缓存中相应的数据失效，保证数据的可见性。

  首先，指令并不是代码行，指令是原子的，通过javap命令可以看到一行代码编译出来的指令，当然，像int i=1;这样的代码行也是原子操作。

  在单例模式中，Instance inst = new Instance();   这一句，就不是原子操作，它可以分成三步原子指令：

  1，分配内存地址；2，new一个Instance对象；3，将内存地址赋值给inst；

  CPU为了提高执行效率，这三步操作的顺序可以是123，也可以是132，如果是132顺序的话，当把内存地址赋给inst后，inst指向的内存地址上面还没有new出来单例对象，这时候，如果就拿到inst的话，它其实就是空的，会报空指针异常。这就是为什么双重检查单例模式中，单例对象要加上volatile关键字。

  内存屏障有三种类型和一种伪类型：
  a、lfence：即读屏障(Load Barrier)，在读指令前插入读屏障，可以让高速缓存中的数据失效，重新从主内存加载数据，以保证读取的是最新的数据。
  b、sfence：即写屏障(Store Barrier)，在写指令之后插入写屏障，能让写入缓存的最新数据写回到主内存，以保证写入的数据立刻对其他线程可见。
  c、mfence，即全能屏障，具备ifence和sfence的能力。
  d、Lock前缀：Lock不是一种内存屏障，但是它能完成类似全能型内存屏障的功能。

### CAS

- CAS包含了三个操作数——需要读写的内存位置V，进行比较的值A和拟写入的新值B。当且仅当V的值等于A时，CAS才会以原子方式用新值B来更新V的值，否则不会执行任何操作。无论位置V的值是否等于A，都将返回V原有的值。 CAS的含义是：我认为V的值应该为A，如果是，那么将V的值更新为B，否则不修改并告诉V的值实际为多少。  上面这段代码模拟了CAS操作（但实际上不是基于synchronized实现的原子操作，而是由操作系统支持的）。 当多个线程尝试使用CAS同时更新一个变量时，只有其中一个线程能更新变量的值，而其他线程都将失败。然而，失败的线程并不会被挂起，而是被告知在这次竞争中失败，并可以再次尝试。由于一个线程在竞争CAS时失败不会被阻塞，因此它可以决定是否重新尝试，或者执行一些恢复操作，也或者不执行任何操作。这种灵活性就大大减少了与锁相关的活跃性风险。

### 死锁

1. 互斥条件：该资源任意一个时刻只由一个线程占用。

2. 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。
3. 不剥夺条件:线程已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。
4. 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。
- 1. 破坏互斥条件 ：这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。2. 破坏请求与保持条件 ：一次性申请所有的资源。3. 破坏不剥夺条件 ：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。4. 破坏循环等待条件 ：靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。                                                        
- 1）避免一个线程同时获取多个锁
2）避免一个线程在锁内占用多个资源，尽量保证每个锁只占用一个资源
3）尝试使用定时锁tryLock替代阻塞式的锁
4）对于数据库锁，加锁和解锁必须在一个数据库连接中，否则会解锁失败

### 线程越多越好吗

- 不是，线程多了可以提高程bai序并行执行的速du度，但是并不是越多越好，其中zhi，每个线程dao都要占用内存，多线程就意味着更多的内存资源被占用，其二，从微观上讲，一个cpu不是同时执行两个线程的，他是轮流执行的，所以线程太多，cpu必须不断的在各个线程间快回更换执行，线程间的切换无意间消耗了许多时间，所以cpu有效利用率反而是下降的

### Semaphore

信号量用来控制同时访问某个特定资源的操作数量，或者同时执行某个指定操作的数量。还可以用来实现某种资源池，或者对容器施加边界。
Semaphore中管理着一组虚拟的许可permit，许可的初始数量可通过构造函数来指定。在执行操作时可以首先获得许可（只要还有剩余的许可），并在使用以后释放许可。如果没有许可，那么acquire将阻塞直到有许可（或者直到被中断或者操作超时）。release方法将返回一个许可给信号量。计算信号量的一种简化形式是二值信号量，即初始值为1的Semaphore。二值信号量可以用作互斥体，并具备不可重入的加锁语义：谁拥有这个唯一的许可，谁就拥有了互斥锁。
可以用于实现资源池，当池为空时，请求资源将会阻塞，直至存在资源。将资源返回给池之后会调用release释放许可。

### AQS

AQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。
CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。

### LockSupprt

当需要阻塞或唤醒一个线程的时候，都会使用LockSupport工具类来完成相应工作。 LockSupport定义了一组公共静态方法，这些方法提供了最基本的线程阻塞和唤醒功能，而LockSupport也成为构建同步组件的基础工具。 LockSupport定义了一组以park开头的方法用来阻塞当前线程，以及unpark(thread)方法来唤醒一个被阻塞的线程。  park等方法还可以传入阻塞对象，有阻塞对象的park方法在dump线程时可以给开发人员更多的现场信息。

park对于中断只会设置中断标志位，不会抛出InterruptedException。 LockSupport是可不重入的，如果一个线程连续2次调用 LockSupport .park()，那么该线程一定会一直阻塞下去 unpark函数可以先于park调用。比如线程B调用unpark函数，给线程A发了一个“许可”，那么当线程A调用park时，它发现已经有“许可”了，那么它会马上再继续运行。

LockSupport.park()和unpark()，与object.wait()和notify()的区别？
主要的区别应该说是它们面向的对象不同。阻塞和唤醒是对于线程来说的，LockSupport的park/unpark更符合这个语义，以“线程”作为方法的参数， 语义更清晰，使用起来也更方便。而wait/notify的实现使得“阻塞/唤醒对线程本身来说是被动的，要准确的控制哪个线程、什么时候阻塞/唤醒很困难， 要不随机唤醒一个线程（notify）要不唤醒所有的（notifyAll）。 park/unpark模型真正解耦了线程之间的同步。线程之间不再须要一个Object或者其他变量来存储状态。不再须要关心对方的状态。

### 线程池

- 1.当线程池小于corePoolSize时，新提交任务将创建一个新线程执行任务，即使此时线程池中存在空闲线程。
2.当线程池达到corePoolSize时，新提交任务将被放入workQueue中，等待线程池中任务调度执行
3.当workQueue已满，且maximumPoolSize>corePoolSize时，新提交任务会创建新线程执行任务
4.当提交任务数超过maximumPoolSize时，新提交任务由RejectedExecutionHandler处理
5.当线程池中超过corePoolSize线程，空闲时间达到keepAliveTime时，关闭空闲线程
6.当设置allowCoreThreadTimeOut(true)时，线程池中corePoolSize线程空闲时间达到keepAliveTime也将关闭
- FixedThreadPool（固定个数的线程池）

	- 这类线程池的特点就是里面全是核心线程，没有非核心线程，也没有超时机制，任务大小也是没有限制的，数量固定，即使是空闲状态，线程不会被回收，除非线程池被关闭，从构造方法也可以看出来，只有两个参数，一个是指定的核心线程数，一个是线程工厂，keepAliveTime无效。任务队列采用了无界的阻塞队列LinkedBlockingQueue，执行execute方法的时候，运行的线程没有达到corePoolSize就创建核心线程执行任务，否则就阻塞在任务队列中，有空闲线程的时候去取任务执行。由于该线程池线程数固定，且不被回收，线程与线程池的生命周期同步，所以适用于任务量比较固定但耗时长的任务。

- SingleThreadPool

	- 适用于需要保证顺序地执行各个任务，并且在任意时间点不会有多个线程活动的应用场景。
	
	- 它也是使用无界队列，corePoolSize和maxPoolSize都为1。
	
	  LinkedBlockingQueue

		- 这类线程池顾名思义就是一个只有一个核心线程的线程池，从构造方法来看，它可以单独执行，也可以与周期线程池结合用。其任务队列是LinkedBlockingQueue，这是个无界的阻塞队列，因为线程池里只有一个线程，就确保所有的任务都在同一个线程中顺序执行，这样就不需要处理线程同步的问题。这类线程池适用于多个任务顺序执行的场景

- CachedThreadPool（）

	- 大小无界的线程池，适用于执行很多的短期异步任务的小程序，或者是负载较轻的服务器

		- 这类线程池的特点就是里面没有核心线程，全是非核心线程，其maximumPoolSize设置为Integer.MAX_VALUE，线程可以无限创建，当线程池中的线程都处于活动状态的时候，线程池会创建新的线程来处理新任务，否则会用空闲的线程来处理新任务，这类线程池的空闲线程都是有超时机制的，keepAliveTime在这里是有效的，时长为60秒，超过60秒的空闲线程就会被回收，当线程池都处于闲置状态时，线程池中的线程都会因为超时而被回收，所以几乎不会占用什么系统资源。任务队列采用的是SynchronousQueue，这个队列是无法插入任务的，一有任务立即执行，所以CachedThreadPool比较适合任务量大但耗时少的任务

- ScheduledThreadPool

	- 固定线程个数，适用于多个后台线程执行周期任务，同时为了满足资源管理的需求而需要限制后台线程的梳理的应用场景。

- 线程池调参

	- CPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1，比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。
I/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。

- FixedThreadPool设置成1 与 SingleThreadPool的区别

	- newSingleThreadExecutor()与newFixedThreadPool(1)的区别就是newSingleThreadExecutor()是被FinalizableDelegatedExecutorService包装了一下
它是线程池的一个代理模式的实现，对线程池所有方法的调用其实是被到了委托类上，不过委托的功能是被阉割的， 因为只实现并委托了部分方法，真实线程池存在的那些未被委托的方法在这里将无法使用。那么这样的目的是什么呢？JDK的源码注释是这么写的：
```java
Unlike the otherwise equivalent {@code newFixedThreadPool(1)} 
the returned executor is guaranteed not to be reconfigurable to use additional threads
```
意思是 newSingleThreadExecutor 不能通过重新配置加入线程，即：

```java
final ExecutorService single = Executors.newSingleThreadExecutor();
ThreadPoolExecutor executor = (ThreadPoolExecutor) single;
// 报错
executor.setCorePoolSize(5);
```

这样就无法在创建了newSingleThreadExecutor以后再把它重新设置为多线程池，保证了单线程池一旦被创建就永远只能为单线程。

### synchronized

- 修饰实例方法相当于当前对象加锁
修饰静态方法相当于当前类对象加锁会作用于类的类的所有对象实例
修饰代码块，指定加锁对象，对给定对象加锁
synchronized  不要用string常量 Integer Long这些
- 对于同步方法，JVM采用ACC_SYNCHRONIZED标记符来实现同步。 对于同步代码块。JVM采用monitorenter、monitorexit两个指令来实现同步。
大致内容如下： 可以把执行monitorenter指令理解为加锁，执行monitorexit理解为释放锁。 每个对象维护着一个记录着被锁次数的计数器。未被锁定的对象的该计数器为0，当一个线程获得锁（执行monitorenter）后，该计数器自增变为 1 ，当同一个线程再次获得该对象的锁的时候，计数器再次自增。当同一个线程释放锁（执行monitorexit指令）的时候，计数器再自减。当计数器为0的时候。锁将被释放，其他线程便可以获得锁。
- moniter对象锁：
包括owner 指向被锁对象头
entryset：存放所有blocking的线程
waitset：指向之前获得锁但是条件不满足进入waiting的线程
- 锁优化

  - 偏向锁

  	- 轻量级锁在没有竞争的时候，每次重入也需要cas操作，jdk1.6之后引入了偏向锁，之后第一次使用CAS 将线程ID设置到对象头mark word中，之后发现这个线程id是自己的就说明是锁重入，不用重新CAS，只要不竞争这个对象就归该线程所有。

  - 轻量级锁

  	- 1：常见锁记录，每个线程的创建栈帧都会包含一个锁记录结构，里面可以存储锁定对象的mark word。
  2：让锁记录的object reference指向锁对象，并且尝试CAS替换Object的mark word，将mark word值存入锁记录
  3：如果CAS失败，如果是其他线程持有了该锁则进行锁膨胀到重量级锁   如果是自己执行了synchronized锁重入则在加入一条锁记录。
  4：在退出synchronized块进行解锁 也是进行cas将锁记录里的mark word恢复到对象头 成功解锁成功  失败索命进行了锁膨胀进行重量级锁的解锁流程。

  - 重量级锁

    - 如果在尝试加轻量级锁的过程中，cas操作无法成功，这时一种情况就是其他线程为此对象加上了轻量级锁（有竞争），这时需要进行锁膨胀，将轻量级锁变为重量级锁。
      重量级锁也有响应的优化
      自旋优化

      重量级锁竞争的手，还可以使用自旋来进行优化，如果当前线程自旋成功 这时当前线程就可以避免阻塞了
      在java6之后自旋锁是自适应的，比如对象刚刚的一次自旋操作成功过，那么认为这次自旋的可能性很高，多次自旋几次；反之，就少自旋甚至不自旋，总之，比较智能。
      自旋会占用CPU时间，单核CPU自旋就是浪费，多核CPU自旋才能发挥优势。
      java7之后不能控制是够开启自旋功能（默认是开启的）

- synchronized与原子性

	- 所以在多线程场景下，由于时间片在线程间轮换，就会发生原子性问题。
通过monitorenter和monitorexit指令，可以保证被synchronized修饰的代码在同一时间只能被一个线程访问，在锁未释放之前，无法被其他线程访问到

- synchronized与可见性

	- synchronized 加锁和解锁的语义：当获取锁以后会清空锁块内本地内存中将会被用到的共享变量，在使用这些共享变量的时候从主内存进行加载，在释放锁时将本地内存中修改的共享变量刷新到主内存中。

- synchronized与有序性

	- synchronized是无法禁止指令重排和处理器优化的
as-if-serial语义保证了单线程中，指令重排是有一定的限制的，而只要编译器和处理器都遵守了这个语义，那么就可以认为单线程程序是按照顺序执行的。当然，实际上还是有重排的，只不过我们无须关心这种重排的干扰。
所以呢，由于synchronized修饰的代码，同一时间只能被同一线程访问。那么也就是单线程执行的。所以，可以保证其有序性。

- synchronized与ReentrantLock

	- ① 两者都是可重入锁
② synchronized 依赖于 JVM，而 ReentrantLock 依赖于 API
③ ReentrantLock 比 synchronized 增加了一些高级功能

相比 synchronized，ReentrantLock 增加了一些高级功能。主要有三点：① 等待可中断；② 可实现公平锁；③ 可实现选择性通知（锁可以绑定多个条件）： 

执行时间长的用线程 用synchronized会好点   执行时间短且线程少 可以用lock(即自旋锁)

## 高并发

### RPC

- 远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。比如两个不同的服务 A、B 部署在两台不同的机器上，那么服务 A 如果想要调用服务 B 中的某个方法该怎么办呢？使用 HTTP请求 当然可以，但是可能会比较慢而且一些优化做的并不好。 RPC 的出现就是为了解决这个问题。
- 服务消费方（client）调用以本地调用方式调用服务；
client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；
client stub找到服务地址，并将消息发送到服务端；
server stub收到消息后进行解码；
server stub根据解码结果调用本地的服务；
本地服务执行并将结果返回给server stub；
server stub将返回结果打包成消息并发送至消费方；
client stub接收到消息，并进行解码；
服务消费方得到最终结果。

### 分布式

- 分布式或者说 SOA 分布式重要的就是面向服务，说简单的分布式就是我们把整个系统拆分成不同的服务然后将这些服务放在不同的服务器上减轻单体服务的压力提高并发量和性能。比如电商系统可以简单地拆分成订单系统、商品系统、登录系统等等，拆分之后的每个服务可以部署在不同的机器上，如果某一个服务的访问量比较大的话也可以将这个服务同时部署在多台机器上。
- 分布式锁

	- 在分布式系统环境下，一个方法在同一时间只能被一个机器的一个线程执行；
	高可用、高性能的获取锁与释放锁；
	具备可重入特性；
	具备锁失效机制、防止死锁；
	具备非阻塞锁特性，即没有获取到锁直接返回获取锁失败；
	- 分布式的CAP理论告诉我们任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）,最多只能同时满足两项。
	一般情况下，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证最终一致性，只要这个最终时间是在用户可以接受的范围内即可
	- 实现方式

		- 数据库

			- 在数据库中创建一个表，表中包含方法名等字段，并在方法名字段上创建唯一索引，想要执行某个方法，就使用这个方法名向表中插入数据，成功插入则获取锁，执行完成后删除对应的行数据释放锁。
			- 问题：数据库要有高可用  不具备重入性 没有锁失效机制 不具备阻塞的特效

		- redis

			- SET lock_key random_value NX PX 5000
			   值得注意的是：random_value 是客户端生成的唯一的字符串。
         NX 代表只在键不存在时，才对键进行设置操作。
         PX 5000 设置键的过期时间为5000毫秒。

		- zookeeper
			- 每个客户端对某个方法加锁时，在zookeeper上的该方法对应的指定节点目录下，生成一个唯一的瞬时有序节点。判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。如果获取到比自己小的兄弟节点不存在，则说明当前线程顺序号最小，获得锁。
			如果判断自己不是那最小的一个节点，则设置监听比自己次小的节点；
			如果已处理完成，则删除自己的节点。

- 分布式事务

  - 本地消息表

  	- 实现：业务处理服务在业务事务提交之前，向实时消息服务请求发送消息，实时消息服务只记录消息数据，而不是真正的发送。业务处理服务在业务事务提交之后，向实时消息服务确认发送。只有在得到确认发送指令后，实时消息服务才会真正发送。
  	消息：业务处理服务在业务事务回滚后，向实时消息服务取消发送。消息发送状态确认系统定期找到未确认发送或者回滚发送的消息，向业务处理服务询问消息状态，业务处理服务根据消息ID或者消息内容确认该消息是否有效。被动方的处理结果不会影响主动方的处理结果，被动方的消息处理操作是幂等操作。
  	成本：可靠的消息系统建设成本，一次消息发送需要两次请求，业务处理服务需要实现消息状态回查接口。
  	优点：消息数据独立存储，独立伸缩，降低业务系统和消息系统之间的耦合。对最终一致性时间敏感度较高，降低业务被动方的实现成本。兼容所有实现JMS标准的MQ中间件，确保业务数据可靠的前提下，实现业务的最终一致性，理想状态下是准实时的一致性。
  	- 优点： 一种非常经典的实现，避免了分布式事务，实现了最终一致性。
  	- 缺点： 消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。

  - TCC事务补偿型

  	- 在分布式事务操作的一方完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中。
  之后将本地消息表中的消息转发到 Kafka 等消息队列中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发。
  在分布式事务操作的另一方从消息队列中读取一个消息，并执行消息中的操作。
  	- 优点： 跟2PC比起来，实现以及流程相对简单了一些，但数据的一致性比2PC也要差一些
  	- 缺点： 缺点还是比较明显的，在2,3步中都有可能失败。TCC属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用TCC不太好定义及处理。

  - 两段提交（2PC）

    - 1：准备提交  1.2 提交阶段

    - 问题

      2.1 同步阻塞 所有事务参与者在等待其它参与者响应的时候都处于同步阻塞状态，无法进行其它操作。
      2.2 单点问题 协调者在 2PC 中起到非常大的作用，发生故障将会造成很大影响。特别是在阶段二发生故障，所有参与者会一直等待状态，无法完成其它操作。
      2.3 数据不一致 在阶段二，如果协调者只发送了部分 Commit 消息，此时网络发生异常，那么只有部分参与者接收到 Commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。
      2.4 太过保守 任意一个节点失败就会导致整个事务失败，没有完善的容错机制

- 分布式ID

	- 雪花算法

		- 组成：一位标识位，41个比特位的时间戳，10位的机器位，可以标识1024台机器，还有就是10比特位的自增序列化
优点：趋势递增；不依赖第三方组件（数据库等），性能更高；可以根据自身ID动态分配比特位
缺点：强依赖机器时钟，如果出现时钟回拨，那么整个系统不可用状态

	- leaf

		- 号段模式

			- Leaf Server 1：从DB加载号段[1，1000]。
Leaf Server 2：从DB加载号段[1001，2000]。
Leaf Server 3：从DB加载号段[2001，3000]
			- 优缺点：
优点：Leaf服务可以很方便的线性拓展，ID是趋势递增的，通过内部有号段缓存，数据库挂了依旧能够支持一段时间，可以自定义max_id大小
缺点: ID不够随机，DB宕机会造成数据不可
优化手段：
采用双buffer 方法，在第一个号段下发了一定的百分比，那么就就会有另一个线程启动来更新下一个号段的缓存数据
每次请求过来的时候都会判断下一个请求的号段状态来更新下一个号段的缓存数据
每个biz_tag都会有监控

		- leaf-snowflow

			- 构成：按照snowflake算法，即是“1+41+10+12”的方式组装ID号，
对于10位的workID是通过zookeeper的顺序节点的特性自动对snowflake节点配置workID；
启动leaf-snowflake服务，连接zookeeper，在leaf_forever下检查是否有注册过，如果注册过，则取回自己的workerID，如果没有注册过，则先在该节点下面创建一个持久化顺序节点，创建成功以后拿到workID；同时本地会缓存对应的workerID
时钟问题：
对于是否插入过该节点的时候，如果已经插入了，那么判断该节点下存储的时间是不是和阈值相匹配，如果超过阈值则认为机器回拨
如果未写过，则是新节点，直接创建持久化顺序节点以后，并写入系统时间，接下来判断自身节点和其他节点是不是一致的， 是通过获取所有运行中的临时节点，通过RPC调用获取到其他服务器的节点
如果差值在阈值以内，认为时间准确，启动服务，并写入临时节点
每隔一段时间上报自身系统时间写入到zk中

	- UUID

		- 优点：性能好，本地生成
缺点：不易存储，信息不安全；如果是使用数据库存储的话，作为主键在INNODB引擎情况下，无序性会使得性能很低

	- 数据库自增ID

		- 优点：容易存储，可以直接用数据库存储，性能好
缺点：信息不安全，递增性太强，涉及到分库操作的时候不好处理，强依赖DB
优化：
多部署几台，定义好步长，每台机器设定好固定的步长；这种方案优点是可以解决强依赖DB问题，缺点就是扩展困难

### zookpeer

- zk的核心是原子广播，这个机制保证了各个Server之间的同步，实现这个机制的协议叫做Zab协议。Zab协议有两种模式，分别是恢复模式（选主）和广播模式（同步）。当服务启动或者leader崩溃后，Zab进入恢复模式，当leader被选举出来，然后进行同步模式，同步完成以后，恢复模式结束。
	为了保证事务的顺序一致性。实现中zxid是一个64位的数字，它高32位是用epoch用来标志leader关系是否改变，每次一个新的leader选举出来，都会拥有一个新的epoch。低32位用来递增计数。
	1. Serverid：在配置server时，给定的服务器的标识id，也就是myid

	2. Zxid：事务id，用来唯一标识一次服务器状态的变更，在某一时刻，集群中的每台机器的zxid值不一定全都一致，zxid越大，表示数据越新。

	3. Epoch：选举的轮数，即逻辑时钟。随着选举的轮数++
- 选举
	- 服务器集群初始化启动
	服务器运行期间无法和Leader保持连接
	leader挂了
	- 当leader崩溃或者leader失去大多数的follower，这时候zk进入恢复模式，然后需要重新选举出一个leader。让所有的Server都恢复到一个正确的状态。Zk选举算法有两种，一种是基于basic paxos实现，一种是基于fast paxos算法实现。系统默认的是fast paxos。
	每个Server在工作过程中有三种状态：
	LOOKING：当前Server不知道Leader是谁，正在投票、选举。
	LEADING：领导者状态。
	FOLLOWING：跟随者状态
	- 首先它会比较 ZXID ，ZXID 大的优先为 Leader，如果相同则比较 myid，myid 大的优先作为 Leader

- wacher

	- Watcher 为事件监听器，是 zk 非常重要的一个特性，很多功能都依赖于它，它有点类似于订阅的方式，即客户端向服务端 注册 指定的 watcher ，当服务端符合了 watcher 的某些事件或要求则会 向客户端发送事件通知 ，客户端收到通知后找到自己定义的 Watcher 然后 执行相应的回调方法 。



## redis

### 数据结构

- string

	- Redis 中的字符串是一种 动态字符串，这意味着使用者可以修改，它的底层实现有点类似于 Java 中的 ArrayList，有一个字符数组，从源码的 sds.h/sdshdr 文件 中可以看到 Redis 底层对于字符串的定义 SDS，即 Simple Dynamic String 结构
	- string 是最常用的一种数据类型，普通的key/value存储都可以归结为string类型，value不仅是string，也可以是数字。其他几种数据类型的构成元素也都是字符串，注意Redis规定字符串的长度不能超过512M
编码 字符串对象的编码可以是int raw embstr
int编码
保存的是可以用long类型表示的整数值
raw编码
保存长度大于44字节的字符串
embstr编码
保存长度小于44字节的字符串
int用来保存整数值，raw用来保存长字符串，embstr用来保存短字符串。embstr编码是用来专门保存短字符串的一种优化编码。
Redis中对于浮点型也是作为字符串保存的，在需要时再将其转换成浮点数类型
编码的转换
当 int 编码保存的值不再是整数，或大小超过了long的范围时，自动转化为raw
对于 embstr 编码，由于 Redis 没有对其编写任何的修改程序（embstr 是只读的），在对embstr对象进行修改时，都会先转化为raw再进行修改，因此，只要是修改embstr对象，修改后的对象一定是raw的，无论是否达到了44个字节。
	- 因为string类型是二进制安全的，可以用来存放图片，视频等内容。
由于redis的高性能的读写功能，而string类型的value也可以是数字，可以用做计数器（使用INCR，DECR指令）。比如分布式环境中统计系统的在线人数，秒杀等。
除了上面提到的，还有用于SpringSession实现分布式session
分布式系统全局序列号

- list

	- list列表,它是简单的字符串列表，你可以添加一个元素到列表的头部，或者尾部。
编码
列表对象的编码可以是ziplist（压缩列表）和linkedlist（双端链表）。
编码转换
同时满足下面两个条件时使用压缩列表：
列表保存元素个数小于512个
每个元素长度小于64字节
不能满足上面两个条件使用linkedlist（双端列表）编码
	
	- 实现数据结构
Stack（栈）
LPUSH+LPOP
Queue（队列）
LPUSH + RPOP
Blocking MQ（阻塞队列）
LPUSH+BRPOP
应用场景
实现简单的消息队列
利用LRANGE命令，实现基于Redis的分页功能

- hash

	- hash对象的键是一个字符串类型，值是一个键值对集合
	编码
	hash对象的编码可以是ziplist或者hashtable
	当使用ziplist，也就是压缩列表作为底层实现时，新增的键值是保存到压缩列表的表尾。
	hashtable 编码的hash表对象底层使用字典数据结构，哈希对象中的每个键值对都使用一个字典键值对。Redis中的字典相当于Java里面的HashMap，内部实现也差不多类似，都是通过“数组+链表”的链地址法来解决哈希冲突的，这样的结构吸收了两种不同数据结构的优点。
	编码转换
	当同时满足下面两个条件使用ziplist编码，否则使用hashtable编码
	列表保存元素个数小于512个
	每个元素长度小于64字节
	- 优点
同类数据归类整合存储，方便数据管理，比如单个用户的所有商品都放在一个hash表里面。
相比string操作消耗内存cpu更小
缺点
hash结构的存储消耗要高于单个字符串
过期功能不能使用在field上，只能用在key上
redis集群架构不适合大规模使用
应用场景
对于 hash 数据类型，value 存放的是键值对，比如可以做单点登录存放用户信息。
存放商品信息，实现购物车

- zset
	- 和集合对象相比，有序集合对象是有序的。与列表使用索引下表作为排序依据不同，有序集合为每一个元素设置一个分数（score）作为排序依据。
	编码
	有序集合的编码可以使ziplist或者skiplist
	ziplist编码的有序集合对象使用压缩列表作为底层实现，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员，第二个节点保存元素的分值。并且压缩列表内的集合元素按分值从小到大的顺序进行排列，小的放置在靠近表头的位置，大的放置在靠近表尾的位置。
	skiplist编码的依序集合对象使用zset结构作为底层实现，一个zset结构同时包含一个字典和一个跳跃表

	- 对于 zset 数据类型，有序的集合，可以做范围查找，排行榜应用，取 TOP N 操作等。

- set

	- 集合对象set是string类型（整数也会转成string类型进行存储）的无序集合。注意集合和列表的区别：集合中的元素是无序的，因此不能通过索引来操作元素；集合中的元素不能有重复。
编码
集合对象的编码可以是intset或者hashtable
intset编码的集合对象使用整数集合作为底层实现，集合对象包含的所有元素都被保存在整数集合中。
hashtable编码的集合对象使用字典作为底层实现，字典的每个键都是一个字符串对象，这里的每个字符串对象就是一个集合中的元素，而字典的值全部设置为null。当使用HT编码时，Redis中的集合SET相当于Java中的HashSet，内部的键值对是无序的，唯一的。内部实现相当于一个特殊的字典，字典中所有value都是NULL。
编码转换
当集合满足下列两个条件时，使用intset编码：
集合对象中的所有元素都是整数
集合对象所有元素数量不超过512
- 对于 set 数据类型，由于底层是字典实现的，查找元素特别快，另外set 数据类型不允许重复，利用这两个特性我们可以进行全局去重，比如在用户注册模块，判断用户名是否注册；微信点赞，微信抽奖小程序
	另外就是利用交集、并集、差集等操作，可以计算共同喜好，全部的喜好，自己独有的喜好，可能认识的人等功能。

- **类型选择**
  string： 存储一个单独的字符串 记录一个商品被访问的次数。set(key,value)
  list： 项目中如果出现了排队的情况，lpush pop
  hash： 通常来存储Java对象 hset(key,feild,value),便于对单个属性的修改
  set： 通常用在，获取两个结果集的交集，补集，差集。
  zSet： 通常用来做排序。
  
  hash：增删改减少了IO次数，
  String：查询减少了IO次数

### 内存回收/内存共享

- 内存回收 因为c语言不具备自动内存回收功能，当将redisObject对象作为数据库的键或值而不是作为参数存储时其生命周期是非常长的，为了解决这个问题，Redis自己构建了一个内存回收机制，通过redisobject结构中的refcount实现.这个属性会随着对象的使用状态而不断变化。
创建一个新对象，属性初始化为1
对象被一个新程序使用，属性refcount加1
对象不再被一个程序使用，属性refcount减1
当对象的引用计数值变为0时，对象所占用的内存就会被释放
- 内存共享 refcount属性除了能实现内存回收以外，还能实现内存共享
将数据块的键的值指针指向一个现有值的对象
将被共享的值对象引用refcount加1 Redis的共享对象目前只支持整数值的字符串对象。之所以如此，实际上是对内存和CPU（时间）的平衡：共享对象虽然会降低内存消耗，但是判断两个对象是否相等却需要消耗额外的时间。对于整数值，判断操作复杂度为o(1),对于普通字符串，判断复杂度为o(n);而对于哈希，列表，集合和有序集合，判断的复杂度为o(n^2).虽然共享的对象只能是整数值的字符串对象，但是5种类型都可能使用共享对象。

### 过期删除/内存淘汰机制

- 过期删除（定期删除+惰性删除。）

	- 定期删除：redis默认是每隔 100ms 就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。注意这里是随机抽取的。为什么要随机呢？你想一想假如 redis 存了几十万个 key ，每隔100ms就遍历所有的设置过期时间的 key 的话，就会给 CPU 带来很大的负载！
惰性删除 ：定期删除可能会导致很多过期 key 到了时间并没有被删除掉。所以就有了惰性删除。假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，除非你的系统去查一下那个 key，才会被redis给删除掉。这就是所谓的惰性删除，也是够懒的哈！
但是仅仅通过设置过期时间还是有问题的。我们想一下：如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期key堆积在内存里，导致redis内存块耗尽了。怎么解决这个问题呢？ redis 内存淘汰机制

- 内存淘汰

	- volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰
volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰
volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰
allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key（这个是最常用的）
allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰
no-eviction：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！
4.0版本后增加以下两种：

volatile-lfu：从已设置过期时间的数据集(server.db[i].expires)中挑选最不经常使用的数据淘汰
allkeys-lfu：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的key

### 持久化

- RBD

	- Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。
	- Redis 在持久化时会调用 glibc 的函数 fork 产生一个子进程，简单理解也就是基于当前进程 复制 了一个进程，主进程和子进程会共享内存里面的代码块和数据段：
	- 优缺点：RDB 持久化优点
RDB是一个紧凑压缩的二进制文件，存储效率高
RDB存储的是Redis在某个时间点的数据快照，非常适用于数据备份全量复制等场景
RDB恢复数据速度比AOF快
RDB应用  ：服务器中每X小时执行bgsave备份，并将RDB文件拷贝到远程机器中，用于灾难恢复
RDB持久化缺点：
RDB方式无论是执行命令还是进行配置，无法做到实时持久化，具有较大可能丢失数据
bgsave每次运行要执行fork操作创建子进程，要牺牲一些性能
Redis的众多版本中未进行RDB文件格式的版本统一，有可能出现各个版本服务器之间数据格式无法兼容
存储数量较大时，效率较低
大数据量下的I／O性能较低
基于fork创建子进程，内存产生额外消耗
宕机带来的数据丢失风险

- AOF

	- AOF(Append Only File - 仅追加文件) 它的工作方式非常简单：每次执行 修改内存 中数据集的写操作时，都会 记录 该操作。假设 AOF 日志记录了自 Redis 实例创建以来 所有的修改性指令序列，那么就可以通过对一个空的 Redis 实例 顺序执行所有的指令，也就是 「重放」，来恢复 Redis 当前实例的内存数据结构的状态。
	- 重写

		- Redis 在长期运行的过程中，AOF 的日志会越变越长。如果实例宕机重启，重放整个 AOF 日志会非常耗时，导致长时间 Redis 无法对外提供服务。所以需要对 AOF 日志 "瘦身"。
Redis 提供了 bgrewriteaof 指令用于对 AOF 日志进行瘦身。其 原理 就是 开辟一个子进程 对内存进行 遍历 转换成一系列 Redis 的操作指令，序列化到一个新的 AOF 日志文件 中。序列化完毕后再将操作期间发生的 增量 AOF 日志 追加到这个新的 AOF 日志文件中，追加完毕后就立即替代旧的 AOF 日志文件了，瘦身工作就完成了。

	- AOF策略使用everysec，每秒fsync一次，该策略仍可保持很好性能，出现问题最多丢失一秒内的数据
数据呈现阶段有效性，建议使用RDB持久化方案
数据可以做到阶段内无丢失，且恢复较快，阶段点数据恢复通常使用RDB方案
注意：
AOF文件存储体积较大，恢复速度较慢
利用RDB使用线紧凑的数据持久化会使Redis性能降低
RDB机制的策略

- 4.0持久化优化

	- 重启 Redis 时，我们很少使用 rdb 来恢复内存状态，因为会丢失大量数据。我们通常使用 AOF 日志重放，但是重放 AOF 日志性能相对 rdb 来说要慢很多，这样在 Redis 实例很大的情况下，启动需要花费很长的时间。
Redis 4.0 为了解决这个问题，带来了一个新的持久化选项——混合持久化。将 rdb 文件的内容和增量的 AOF 日志文件存在一起。这里的 AOF 日志不再是全量的日志，而是 自持久化开始到持久化结束 的这段时间发生的增量 AOF 日志，通常这部分 AOF 日志很

### 事务

- Redis 事务的本质是一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。
总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。
- 开始事务
命令入队
执行事务
- Redis事务没有隔离级别的概念：
批量操作在发送 EXEC 命令前被放入队列缓存，并不会被实际执行，也就不存在事务内的查询要看到事务里的更新，事务外查询不能看到。
Redis不保证原子性：

　　Redis中，单条命令是原子性执行的，但事务不保证原子性，且没有回滚。事务中任意命令执行失败，其余的命令仍会被执行。
- watch key1 key2 ... : 监视一或多个key,如果在事务执行之前，被监视的key被其他命令改动，则事务被打断 （ 类似乐观锁 ）

　　multi : 标记一个事务块的开始（ queued ）

　　exec : 执行所有事务块的命令 （ 一旦执行exec后，之前加的监控锁都会被取消掉 ）　

　　discard : 取消事务，放弃事务块中的所有命令

　　unwatch : 取消watch对所有key的监控

### redis集群

- 主从

  - master可以拥有多个slave
    2、多个slave可以连接到同一个master，还可以连接到其他的slave
    3、主从复制不会阻塞master，master仍可以接收客户端请求
    4、提高系统的伸缩性

  - 数据冗余： 主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。

    故障恢复： 当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复 (实际上是一种服务的冗余)。

    负载均衡： 在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务 （即写 Redis 数据时应用连接主节点，读 Redis 数据时应用连接从节点），分担服务器负载。尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高 Redis 服务器的并发量。

    高可用基石： 除了上述作用以外，主从复制还是哨兵和集群能够实施的 基础，因此说主从复制是 Redis 高可用的基础。

    ![image-20210730105953163](https://raw.githubusercontent.com/lxcsjk/oss/master/uPic/image-20210730105953163.png)
- SYNC 命令是一个非常耗费资源的操作

每次执行 SYNC 命令，主从服务器需要执行如下动作：
1. 主服务器 需要执行 BGSAVE 命令来生成 RDB 文件，这个生成操作会 消耗 主服务器大量的 CPU、内存和磁盘 I/O 的资源；
2. 主服务器 需要将自己生成的 RDB 文件 发送给从服务器，这个发送操作会 消耗 主服务器 大量的网络资源 (带宽和流量)，并对主服务器响应命令请求的时间产生影响；
3. 接收到 RDB 文件的 从服务器 需要载入主服务器发来的 RBD 文件，并且在载入期间，从服务器 会因为阻塞而没办法处理命令请求；
特别是当出现 断线重复制 的情况是时，为了让从服务器补足断线时确实的那一小部分数据，却要执行一次如此耗资源的 SYNC 命令，显然是不合理的。
PSYNC 命令的引入

所以在 Redis 2.8 中引入了 PSYNC 命令来代替 SYNC，它具有两种模式：
1. 全量复制： 用于初次复制或其他无法进行部分复制的情况，将主节点中的所有数据都发送给从节点，是一个非常重型的操作；
2. 部分复制： 用于网络中断等情况后的复制，只将 中断期间主节点执行的写命令 发送给从节点，与全量复制相比更加高效。需要注意 的是，如果网络中断时间过长，导致主节点没有能够完整地保存中断期间执行的写命令，则无法进行部分复制，仍使用全量复制；
部分复制的原理主要是靠主从节点分别维护一个 复制偏移量，有了这个偏移量之后断线重连之后一比较，之后就可以仅仅把从服务器断线之后确实的这部分数据给补回来了。

- 哨兵模式

  - 哨兵节点： 哨兵系统由一个或多个哨兵节点组成，哨兵节点是特殊的 Redis 节点，不存储数据；
  - 数据节点： 主节点和从节点都是数据节点；
    在复制的基础上，哨兵实现了 自动化的故障恢复 功能，下方是官方对于哨兵功能的描述：
  - 监控（Monitoring）： 哨兵会不断地检查主节点和从节点是否运作正常。
  - 自动故障转移（Automatic failover）： 当 主节点 不能正常工作时，哨兵会开始 自动故障转移操作，它会将失效主节点的其中一个 从节点升级为新的主节点，并让其他从节点改为复制新的主节点。
  - 配置提供者（Configuration provider）： 客户端在初始化时，通过连接哨兵来获得当前 Redis 服务的主节点地址。
  - 通知（Notification）： 哨兵可以将故障转移的结果发送给客户端。
    其中，监控和自动故障转移功能，使得哨兵可以及时发现主节点故障并完成转移。而配置提供者和通知功能，则需要在与客户端的交互中才能体现。
- 选举

	- 1. 在失效主服务器属下的从服务器当中， 那些被标记为主观下线、已断线、或者最后一次回复 PING 命令的时间大于五秒钟的从服务器都会被 淘汰。
	  2. 在失效主服务器属下的从服务器当中， 那些与失效主服务器连接断开的时长超过 down-after 选项指定的时长十倍的从服务器都会被 淘汰。
	  3. 在 经历了以上两轮淘汰之后 剩下来的从服务器中， 我们选出 复制偏移量（replication offset）最大 的那个 从服务器 作为新的主服务器；如果复制偏移量不可用，或者从服务器的复制偏移量相同，那么 带有最小运行 ID 的那个从服务器成为新的主服务器。

- cluster

	- Redis 集群中内置了 16384 个哈希槽。当客户端连接到 Redis 集群之后，会同时得到一份关于这个 集群的配置信息，当客户端具体对某一个 key 值进行操作时，会计算出它的一个 Hash 值，然后把结果对 16384  求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，Redis 会根据节点数量 大致均等 的将哈希槽映射到不同的节点。
	- 1. 数据分区： 数据分区 (或称数据分片) 是集群最核心的功能。集群将数据分散到多个节点，一方面 突破了 Redis 单机内存大小的限制，存储容量大大增加；另一方面 每个主节点都可以对外提供读服务和写服务，大大提高了集群的响应能力。Redis 单机内存大小受限问题，在介绍持久化和主从复制时都有提及，例如，如果单机内存太大，bgsave 和 bgrewriteaof 的 fork 操作可能导致主进程阻塞，主从环境下主机切换时可能导致从节点长时间无法提供服务，全量复制阶段主节点的复制缓冲区可能溢出……
	  2. 高可用： 集群支持主从复制和主节点的 自动故障转移 （与哨兵类似），当任一节点发生故障时，集群仍然可以对外提供服务。
### 数据分区

- 一致性hash

	- 一致性哈希算法将 整个哈希值空间 组织成一个虚拟的圆环，范围是 [0 , 232-1]，对于每一个数据，根据 key 计算 hash 值，确数据在环上的位置，然后从此位置沿顺时针行走，找到的第一台服务器就是其应该映射到的服务器：

- 哈希值 % 节点数

	- 哈希取余分区思路非常简单：计算 key 的 hash 值，然后对节点数量进行取余，从而决定数据映射到哪个节点上。
不过该方案最大的问题是，当新增或删减节点时，节点数量发生变化，系统中所有的数据都需要 重新计算映射关系，引发大规模数据迁移。

- 带有虚拟节点的一致性哈希分区

  - 该方案在 一致性哈希分区的基础上，引入了 虚拟节点 的概念。Redis 集群使用的便是该方案，其中的虚拟节点称为 槽（slot）。槽是介于数据和实际节点之间的虚拟概念，每个实际节点包含一定数量的槽，每个槽包含哈希值在一定范围内的数据。
    在使用了槽的一致性哈希分区中，槽是数据管理和迁移的基本单位。槽 解耦 了 数据和实际节点 之间的关系，增加或删除节点对系统的影响很小。仍以上图为例，系统中有 4 个实际节点，假设为其分配 16 个槽(0-15)；

    槽 0-3 位于 node1；4-7 位于 node2；以此类推....
    如果此时删除 node2，只需要将槽 4-7 重新分配即可，例如槽 4-5 分配给 node1，槽 6 分配给 node3，槽 7 分配给 node4；可以看出删除 node2 后，数据在其他节点的分布仍然较为均衡。
### 节点通信机制

- 两个端口

	- 节点都存储数据，也都参与集群状态的维护。为此，集群中的每个节点，都提供了两个 TCP 端口：
	
	  普通端口： 即我们在前面指定的端口 (7000等)。普通端口主要用于为客户端提供服务 （与单机节点类似）；但在节点间数据迁移时也会使用。
	
	  集群端口： 端口号是普通端口 + 10000 （10000是固定值，无法改变），如 7000 节点的集群端口为 17000。集群端口只用于节点之间的通信，如搭建集群、增减节点、故障转移等操作时节点间的通信；不要使用客户端连接集群接口。为了保证集群可以正常工作，在配置防火墙时，要同时开启普通端口和集群端口。
- Gossip 协议

	- 节点间通信，按照通信协议可以分为几种类型：单对单、广播、Gossip 协议等。重点是广播和 Gossip 的对比。
	
	  广播是指向集群内所有节点发送消息。优点 是集群的收敛速度快(集群收敛是指集群内所有节点获得的集群信息是一致的)，缺点 是每条消息都要发送给所有节点，CPU、带宽等消耗较大。
	
	  Gossip 协议的特点是：在节点数量有限的网络中，每个节点都 “随机” 的与部分节点通信 （并不是真正的随机，而是根据特定的规则选择通信的节点），经过一番杂乱无章的通信，每个节点的状态很快会达到一致。Gossip 协议的 优点 有负载 (比广播) 低、去中心化、容错性高 (因为通信有冗余) 等；缺点 主要是集群的收敛速度慢。
### 跳跃表

- skiplist和各种平衡树（如AVL、红黑树等）的元素是有序排列的，而哈希表不是有序的。因此，在哈希表上只能做单个key的查找，不适宜做范围查找。所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点。
在做范围查找的时候，平衡树比skiplist操作要复杂。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在skiplist上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现。
平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。
从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势。
查找单个key，skiplist和平衡树的时间复杂度都为O(log n)，大体相当；而哈希表在保持较低的哈希值冲突概率的前提下，查找时间复杂度接近O(1)，性能更高一些。所以我们平常使用的各种Map或dictionary结构，大都是基于哈希表实现的。
从算法实现难度上来比较，skiplist比平衡树要简单得多。
- 跳跃表是为了让链表能够可以二分查找的能力
- 跳跃表 skiplist 就是受到这种多层链表结构的启发而设计出来的。按照上面生成链表的方式，上面每一层链表的节点个数，是下面一层的节点个数的一半，这样查找过程就非常类似于一个二分查找，使得查找的时间复杂度可以降低到 O(logn)。
但是，这种方法在插入数据的时候有很大的问题。新插入一个节点之后，就会打乱上下相邻两层链表上节点个数严格的 2:1 的对应关系。如果要维持这种对应关系，就必须把新插入的节点后面的所有节点 （也包括新插入的节点） 重新进行调整，这会让时间复杂度重新蜕化成 O(n)。删除数据也有同样的问题。
skiplist 为了避免这一问题，它不要求上下相邻两层链表之间的节点个数有严格的对应关系，而是 为每个节点随机出一个层数(level)。比如，一个节点随机出的层数是 3，那么就把它链入到第 1 层到第 3 层这三层链表中。为了表达清楚，下图展示了如何通过一步步的插入操作从而形成一个 skiplist 的过程

### 其他问题

- 大批量导入

	- 当频繁的存储获取Redis数据库中的数据时，可以使用Redis的pipeline(管道)功能，将多个相互没有依赖关系的读写操作，如：下一步执行的Redis操作的开启需要获取上一步操作执行结束的数据。放到队列中，使用pipeline对象一次性执行，可以很大程度上减少与数据库建立TCP连接的性能损耗

- 内存优化

	- 缩减键值对象：满足业务要求下 key 越短越好；value 值进行适当压缩
共享对象池：即 Redis 内部维护[0-9999]的整数对象池，开发中在满足需求的前提下，尽量使用整数对象以节省内存
尽可能使用散列表(hashes)
编码优化，控制编码类型
控制 key 的数量


- 集群写丢失

	- 过期 key 被清理
最大内存不足，导致 Redis 自动清理部分 key 以节省空间
主库故障后自动重启，从库自动同步
单独的主备方案，网络不稳定触发哨兵的自动切换主从节点，切换期间会有数据丢失

- 异步队列

  - 方式一：生产者消费者模式
  使用list结构作为队列，rpush生产消息，lpop消费消息，当lpop没有消息的时候，要适当sleep一会再重试。
  或者，不用sleep，直接用blpop指令，在没有消息的时候，它会阻塞住直到消息到来。
  - 方式二：发布订阅者模式
  使用pub/sub主题订阅者模式，可以实现1:N的消息队列。
  - 缺点：在消费者下线的情况下，生产的消息会丢失。此场景，建议用MQ

- 雪崩

  - 事前：尽量保证整个 redis 集群的高可用性，发现机器宕机尽快补上。选择合适的内存淘汰策略。
  - 事中：本地ehcache缓存 + hystrix限流&降级，避免MySQL崩掉
  - 事后：利用 redis 持久化机制保存的数据尽快恢复缓存
- 滑动窗口

	-  我们可以将请求打造成一个zset数组，当每一次请求进来的时候，value保持唯一，可以用UUID生成，而score可以用当前时间戳表示，因为score我们可以用来计算当前时间戳之内有多少的请求数量。而zset数据结构也提供了range方法让我们可以很轻易的获取到2个时间戳内有多少请求


- 单线程效率高的原因

  1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)；

  2、数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的；

  3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；

  4、使用多路I/O复用模型，非阻塞IO；

  5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求；

## mysql

### 索引

- 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。
可以大大加快 数据的检索速度（大大减少的检索的数据量）, 这也是创建索引的最主要的原因。
帮助服务器避免排序和临时表。
将随机IO变为顺序IO
可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义
- 在经常需要搜索的列上，可以加快搜索的速度；
在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。
在经常需要排序的列上创 建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间；
对于中到大型表索引都是非常有效的，但是特大型表的话维护开销会很大，不适合建索引
在经常用在连接的列上，这 些列主要是一些外键，可以加快连接的速度；
避免 where 子句中对宇段施加函数，这会造成无法命中索引。
在使用InnoDB时使用与业务无关的自增主键作为主键，即使用逻辑主键，而不要使用业务主键。
将打算加索引的列设置为 NOT NULL ，否则将导致引擎放弃使用索引而进行全表扫描
删除长期未使用的索引，不用的索引的存在会造成不必要的性能损耗 MySQL 5.7 可以通过查询 sys 库的 chema_unused_indexes 视图来查询哪些索引从未被使用
在使用 limit offset 查询缓慢时，可以借助索引来提高性能     出现在 SELECT、UPDATE、DELETE 语句的 WHERE 从句中的列
包含在 ORDER BY、GROUP BY、DISTINCT 中的字段
并不要将符合 1 和 2 中的字段的列都建立一个索引， 通常将 1、2 中的字段建立联合索引效果更好
多表 join 的关联列
- b+树

	- B树的所有节点既存放 键(key) 也存放 数据(data);而B+树只有叶子节点存放 key 和 data，其他内节点只存放key。
B树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。
B树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显。

- 聚集非聚集

	- 聚集索引即索引结构和数据一起存放的索引。主键索引属于聚集索引。
	  聚集索引的优点：
	  聚集索引的查询速度非常的快，因为整个B+树本身就是一颗多叉平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。
	  聚集索引的缺点：
	  依赖于有序的数据 ：因为B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或UUID这种又长又难比较的数据，插入或查找的速度肯定比较慢。
	  更新代价大 ： 如果对索引列的数据被修改时，那么对应的索引也将会被修改， 而且况聚集索引的叶子节点还存放着数据，修改代价肯定是较大的， 所以对于主键索引来说，主键一般都是不可被修改的。
	
	- 非聚集索引的叶子节点并不一定存放数据的指针， 因为二级索引的叶子节点就存放的是主键，根据主键再回表查数据。非聚集索引的优点
	
	  更新代价比聚集索引要小 。非聚集索引的更新代价就没有聚集索引那么大了，非聚集索引的叶子节点是不存放数据的
	
	  非聚集索引的缺点
	
	  跟聚集索引一样，非聚集索引也依赖于有序的数据
	  可能会二次查询(回表) :这应该是非聚集索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。

- 覆盖索引

	- 如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。我们知道在InnoDB存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次。这样就会比较慢覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！
覆盖索引即需要查询的字段正好是索引的字段，那么直接根据该索引，就可以查到数据了， 而无需回表查询。
如主键索引，如果一条SQL需要查询主键，那么正好根据主键索引就可以查到主键。
再如普通索引，如果一条SQL需要查询name，name字段正好有索引， 那么直接根据这个索引就可以查到数据，也无需回表。

- join/in/or

	- 对应同一列进行 or 判断时，使用 in 代替 or
in 的值不要超过 500 个，in 操作可以更有效的利用索引，or 大多数情况下很少能利用到索引。
	- 通常子查询在 in 子句中，且子查询中为简单 SQL(不包含 union、group by、order by、limit 从句) 时,才可以把子查询转化为关联查询进行优化。
子查询性能差的原因：
子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。
由于子查询会产生大量的临时表也没有索引，所以会消耗过多的 CPU 和 IO 资源，产生大量的慢查询。
	- 对于 MySQL 来说，是存在关联缓存的，缓存的大小可以由 join_buffer_size 参数进行设置。
在 MySQL 中，对于同一个 SQL 多关联（join）一个表，就会多分配一个关联缓存，如果在一个 SQL 中关联的表越多，所占用的内存也就越大。
如果程序中大量的使用了多表关联的操作，同时 join_buffer_size 设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性。
同时对于关联操作来说，会产生临时表操作，影响查询效率，MySQL 最多允许关联 61 个表，建议不超过 5 个。

### 引擎

- MyISAM是MySQL的默认数据库引擎（5.5版之前）。虽然性能极佳，而且提供了大量的特性，包括全文索引、压缩、空间函数等，但MyISAM不支持事务和行级锁，而且最大的缺陷就是崩溃后无法安全恢复。不过，5.5版本之后，MySQL引入了InnoDB（事务性数据库引擎），MySQL 5.5版本后默认的存储引擎为InnoDB。

  大多数时候我们使用的都是 InnoDB 存储引擎，但是在某些情况下使用 MyISAM 也是合适的比如读密集的情况下。（如果你不介意 MyISAM 崩溃恢复问题的话）。

  两者的对比：
  是否支持行级锁 : MyISAM 只有表级锁(table-level locking)，而InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁。
  是否支持事务和崩溃后的安全恢复： MyISAM 强调的是性能，每次查询具有原子性,其执行速度比InnoDB类型更快，但是不提供事务支持。但是InnoDB 提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。
  是否支持外键： MyISAM不支持，而InnoDB支持。
  是否支持MVCC ：仅 InnoDB 支持。应对高并发事务, MVCC比单纯的加锁更高效;MVCC只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作;MVCC可以使用 乐观(optimistic)锁 和 悲观(pessimistic)锁来实现;各数据库中MVCC实现并不统一。推荐阅读：MySQL-InnoDB-MVCC多版本并发控制
  ......

### sql优化

- 查询优化

  - 2.1 避免在 where 子句中对字段进行 null 值判断，where 子句中对字段进行 null 值判断，这样导致引擎放弃对该字段的索引查询，而变为全表扫描。
   如何解决？我们可以给需要判断的字段设置默认值。
                                                                      如：select id from t where num is null    
                                                                        可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：    
                                                                        select id from t where num=0    

  - 2.2 尽量避免在 where 子句中使用!=或<>操作符使用该符号后，也会导致字段索引无效，进行全表扫描。
  
  - 2.3 应尽量避免在 where 子句中使用 or 来连接条件使用or来连接条件，也会导致放弃使用索引，而进行全表扫描。
  
     解决方案：
      select id from t where num=10 or num=20    
      可以这样查询：    
      select id from t where num=10    union all  select id from t where num=20   
  
  - 2.4 应尽量避免在 where中使用 in 和 not in
    使用in 和 not in，也会导致放弃使用索引，而进行全表扫描。
                                                                                      建议： 对于连续的数值，能用 between 不用 in 了    
                                                                                      select id from t where num in(1,2,3)    改为下面的查询 select id from t where num between 1 and 3   
  - 2.5 应尽量避免使用like关键字作为查询条件
    在like条件中使用的前后都模糊查询，会导致全表查询。如下面的例子 select id from t where name like '%abc%'     
  - 2.6 应尽量避免在where子句中对字段进行函数或表达式操作对字段进行函数或表达式操作，会导致字段的索引失效。进行全表扫描。
    查询条件name以abc开头的id 
                                                                                    select id from t where substring(name,1,3)='abc'   
                                                                                    应改为:    
                                                                                    select id from t where name like 'abc%'    
                                                                                    查询num一半为100的id。
                                                                                    select id from t where num/2=100    
                                                                                    应改为:    
                                                                                    select id from t where num=100*2    
  - 2.7使用索引字段作为条件时，复合索引的使用使用符合索引字段作为条件时，必须使用到该索引中的第一个字段作为条件才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的使用符合索引字段作为条件且顺序与索引顺序相一致。    
  - 2.8很多时候用 exists 代替 in 是一个好的选择
    select num from a where num in(select num from b)    
                                                                                    用下面的语句替换：    
                                                                                    select num from a where exists(select 1 from b where num=a.num)    
  - 2.10尽量避免大事务操作，提高系统并发能力

### 事务

- 特性：
原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；
一致性： 执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的；
隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；
持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。

	- 原子性：
是利用Innodb的undo log。
undo log名为回滚日志，是实现原子性的关键，当事务回滚时能够撤销所有已经成功执行的sql语句，他需要记录你要回滚的相应日志信息。
例如
1、当你delete一条数据的时候，就需要记录这条数据的信息，回滚的时候，insert这条旧数据
2、当你update一条数据的时候，就需要记录之前的旧值，回滚的时候，根据旧值执行update操作
3、当年insert一条数据的时候，就需要这条记录的主键，回滚的时候，根据主键执行delete操作
undo log记录了这些回滚需要的信息，当事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。
	- 持久性：
决定采用redo log解决上面的问题。当做数据修改的时候，不仅在内存中操作，还会在redo log中记录这次操作。当事务提交的时候，会将redo log日志进行刷盘(redo log一部分在内存中，一部分在磁盘上)。当数据库宕机重启的时候，会将redo log中的内容恢复到数据库中，再根据undo log和binlog内容决定回滚数据还是提交数据。
采用redo log的好处？ 其实好处就是将redo log进行刷盘比对数据页刷盘效率高，具体表现如下
redo log体积小，毕竟只记录了哪一页修改了啥，因此体积小，刷盘快。
redo log是一直往末尾进行追加，属于顺序IO。效率显然比随机IO来的快。
	- 隔离性:
利用的是锁和MVCC机制
- 并发事务带来的问题

	- 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。
	- 丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。	例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。
	- 不可重复读（Unrepeatableread）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。
	- 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。
- 事务的隔离级别

	- READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。
	- READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。
	- REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。
	- SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。

### 三大日志

#### binlog

binlog用于记录数据库执行的写入性操作(不包括查询)信息，以二进制的形式保存在磁盘中。binlog是mysql的逻辑日志，并且由Server层进行记录，使用任何存储引擎的mysql数据库都会记录binlog日志。

- 逻辑日志：可以简单理解为记录的就是sql语句。
- 物理日志：因为mysql数据最终是保存在数据页中的，物理日志记录的就是数据页变更。

binlog是通过追加的方式进行写入的，可以通过max_binlog_size参数设置每个binlog文件的大小，当文件大小达到给定值之后，会生成新的文件来保存日志。

##### binlog使用场景

在实际应用中，binlog的主要使用场景有两个，分别是主从复制和数据恢复。

- 主从复制：在Master端开启binlog，然后将binlog发送到各个Slave端，Slave端重放binlog从而达到主从数据一致。
- 数据恢复：通过使用mysqlbinlog工具来恢复数据。

##### binlog刷盘时机

对于InnoDB存储引擎而言，只有在事务提交时才会记录biglog，此时记录还在内存中，那么biglog是什么时候刷到磁盘中的呢？mysql通过sync_binlog参数控制biglog的刷盘时机，取值范围是0-N：

- 0：不去强制要求，由系统自行判断何时写入磁盘；
- 1：每次commit的时候都要将binlog写入磁盘；
- N：每N个事务，才会将binlog写入磁盘。

从上面可以看出，sync_binlog最安全的是设置是1，这也是MySQL 5.7.7之后版本的默认值。但是设置一个大一些的值可以提升数据库性能，因此实际情况下也可以将值适当调大，牺牲一定的一致性来获取更好的性能。

##### binlog日志格式

binlog日志有三种格式，分别为STATMENT、ROW和MIXED。

在 MySQL 5.7.7之前，默认的格式是STATEMENT，MySQL 5.7.7之后，默认值是ROW。日志格式通过binlog-format指定。

###### STATMENT

基于SQL语句的复制(statement-based replication, SBR)，每一条会修改数据的sql语句会记录到binlog中。

- 优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO, 从而提高了性能；
- 缺点：在某些情况下会导致主从数据不一致，比如执行sysdate()、slepp()等。

###### ROW

基于行的复制(row-based replication, RBR)，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了。

- 优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题；
- 缺点：会产生大量的日志，尤其是alter table的时候会让日志暴涨

###### MIXED

基于STATMENT和ROW两种模式的混合复制(mixed-based replication, MBR)，一般的复制使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlog

#### redo log

##### 为什么需要redo log

我们都知道，事务的四大特性里面有一个是持久性，具体来说就是只要事务提交成功，那么对数据库做的修改就被永久保存下来了，不可能因为任何原因再回到原来的状态。

那么mysql是如何保证一致性的呢？最简单的做法是在每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中。但是这么做会有严重的性能问题，主要体现在两个方面：

- 因为Innodb是以页为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，太浪费资源了！
- 一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机IO写入性能太差！

因此mysql设计了redo log，具体来说就是只记录事务对数据页做了哪些修改，这样就能完美地解决性能问题了(相对而言文件更小并且是顺序IO)。

##### redo log基本概念

redo log包括两部分：一个是内存中的日志缓冲(redo log buffer)，另一个是磁盘上的日志文件(redo log file)。mysql每执行一条DML语句，先将记录写入redo log buffer，后续某个时间点再一次性将多个操作记录写到redo log file。

这种先写日志，再写磁盘的技术就是MySQL里经常说到的WAL(Write-Ahead Logging) 技术。

在计算机操作系统中，用户空间(user space)下的缓冲区数据一般情况下是无法直接写入磁盘的，中间必须经过操作系统内核空间(kernel space)缓冲区(OS Buffer)。

因此，redo log buffer写入redo log file实际上是先写入OS Buffer，然后再通过系统调用fsync()将其刷到redo log file中，过程如下：

![img](https://raw.githubusercontent.com/lxcsjk/oss/master/uPic/640.png)

mysql支持三种将redo log buffer写入redo log file的时机，可以通过innodb_flush_log_at_trx_commit参数配置，各参数值含义如下：

![img](https://raw.githubusercontent.com/lxcsjk/oss/master/uPic/640-20210820234846883.png)

![img](https://raw.githubusercontent.com/lxcsjk/oss/master/uPic/640-20210820234850321.png)

###  

##### redo log记录形式

前面说过，redo log实际上记录数据页的变更，而这种变更记录是没必要全部保存，因此redo log实现上采用了大小固定，循环写入的方式，当写到结尾时，会回到开头循环写日志。如下图：

![img](https://raw.githubusercontent.com/lxcsjk/oss/master/uPic/640-20210820234900365.png)

同时我们很容易得知，在innodb中，既有redo log需要刷盘，还有数据页也需要刷盘，redo log存在的意义主要就是降低对数据页刷盘的要求。在上图中，write pos表示redo log当前记录的LSN(逻辑序列号)位置，check point表示数据页更改记录刷盘后对应redo log所处的LSN(逻辑序列号)位置。

write pos到check point之间的部分是redo log空着的部分，用于记录新的记录；check point到write pos之间是redo log待落盘的数据页更改记录。当write pos追上check point时，会先推动check point向前移动，空出位置再记录新的日志。

启动innodb的时候，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。因为redo log记录的是数据页的物理变化，因此恢复的时候速度比逻辑日志(如binlog)要快很多。

重启innodb时，首先会检查磁盘中数据页的LSN，如果数据页的LSN小于日志中的LSN，则会从checkpoint开始恢复。

还有一种情况，在宕机前正处于checkpoint的刷盘过程，且数据页的刷盘进度超过了日志页的刷盘进度，此时会出现数据页中记录的LSN大于日志中的LSN，这时超出日志进度的部分将不会重做，因为这本身就表示已经做过的事情，无需再重做。

![img](https://raw.githubusercontent.com/lxcsjk/oss/master/uPic/640-20210820234917913.png)

##  

#### redo log与binlog区别

由binlog和redo log的区别可知：binlog日志只用于归档，只依靠binlog是没有crash-safe能力的。但只有redo log也不行，因为redo log是InnoDB特有的，且日志上的记录落盘后会被覆盖掉。因此需要binlog和redo log二者同时记录，才能保证当数据库发生宕机重启时，数据不会丢失。

#### undo log

数据库事务四大特性中有一个是原子性，具体来说就是 原子性是指对数据库的一系列操作，要么全部成功，要么全部失败，不可能出现部分成功的情况。

实际上，原子性底层就是通过undo log实现的。undo log主要记录了数据的逻辑变化，比如一条INSERT语句，对应一条DELETE的undo log，对于每个UPDATE语句，对应一条相反的UPDATE的undo log，这样在发生错误时，就能回滚到事务之前的数据状态。

同时，undo log也是MVCC(多版本并发控制)实现的关键，这部分内容在面试中的老大难-mysql事务和锁，一次性讲清楚！中有介绍，不再赘述。

### 数据库锁

- 乐观锁/悲观锁

  - 乐观锁:
    实现：大多数基于数据版本（Version）记录机制实现
         具体可通过给表加一个版本号或时间戳字段实现，当读取数据时，将version字段的值一同读出，数据每更新一次，对此version值加一。当我们提交更新的时候，判断当前版本信息与第一次取出来的版本值大小，如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据，拒绝更新，让用户重新操作。

  - 悲观锁:
    大多数情况下依靠数据库的锁机制实现

     一般使用 select ...for update 对所选择的数据进行加锁处理，例如select * from account where name=”Max” for update， 这条sql 语句锁定了account 表中所有符合检索条件（name=”Max”）的记录。本次事务提交之前（事务提交时会释放事务过程中的锁），外界无法修改这些记录。


- inoodb锁

	- Record lock：单个行记录上的锁
Gap lock：间隙锁，锁定一个范围，不包括记录本身
Next-key lock：record+gap 锁定一个范围，包含记录本身
innodb对于行的查询使用next-key lock
Next-locking keying为了解决Phantom Problem幻读问题
当查询的索引含有唯一属性时，将next-key lock降级为record key
Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生
有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1

- 按照锁粒度分

	- 行级锁（通过对索引加锁实现的） 行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。
特点：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。
	- 表级锁 表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。
特点：开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。
	- 页级锁 页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。
特点：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般

- 死锁

	- 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。
	
	- 常见的解决死锁的方法
	
	  1、如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。
	
	  2、在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率；
	
	  3、对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率；
	
	  如果业务处理不好可以用分布式事务锁或者使用乐观锁

### 其他问题

- limit 10000

	- limit10000,20的意思扫描满足条件的10020行，扔掉前面的10000行，返回最后的20行，问题就在这里。
	
	  1. 用order by 优化-：SELECT id,title,content FROM items WHERE id IN (SELECT id FROM items ORDER BY id limit 900000, 10);
	
	  2. 如果offset过大：SELECT * FROM users WHERE uid >=  (SELECT uid FROM users ORDER BY uid limit 895682, 1) limit 0, 10;


- explain

  - ID: id列中的数据为一组数字，表示执行select语句顺序 id值相同时，执行顺序由上至下id值越大优先级越高，越先被执行
  SELECT_TYPE:查询类型-简单查询 联合查询 子查询 复杂查询
  - TYPE :访问类型，sql查询优化中一个很重要的指标，结果值从好到坏依次是：
    system > const > eq_ref > ref > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL
    - system 这是const联接类型的一个特例，当查询的表只有一行时使用
    - const：表示通过索引一次就找到了，const用于比较primary key 或者 unique索引。因为只需匹配一行数据，所有很快。如果将主键置于where列表中，mysql就能将该查询转换为一个const
    - eq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配。常见于主键 或 唯一索引扫描。
    - ref：非唯一性索引扫描，返回匹配某个单独值的所有行。本质是也是一种索引访问，它返回所有匹配某个单独值的行，然而他可能会找到多个符合条件的行，所以它应该属于查找和扫描的混合体
    - ref_or_null 类似于 ref类型的查询，但是附加了对 null 值列的查询
    - range：只检索给定范围的行，使用一个索引来选择行。key列显示使用了那个索引。一般就是在where语句中出现了between、<、>、in等的查询。这种索引列上的范围扫描比全索引扫描要好。只需要开始于某个点，结束于另一个点，不用扫描全部索引
    - index：Full Index Scan，index与ALL区别为index类型只遍历索引树。这通常为ALL块，应为索引文件通常比数据文件小。（Index与ALL虽然都是读全表，但index是从索引中读取，而ALL是从硬盘读取）
    - ALL：Full Table Scan，遍历全表以找到匹配的行

- Mysql某个表有近千万数据，CRUD比较慢

	- 可以做表拆分，减少单表字段数量，优化表结构。
在保证主键有效的情况下，检查主键索引的字段顺序，使得查询语句中条件的字段顺序和主键索引的字段顺序保持一致。
主要两种拆分 垂直拆分，水平拆分。
垂直分表
也就是“大表拆小表”，基于列字段进行的。一般是表中的字段较多，将不常用的， 数据较大，长度较长（比如text类型字段）的拆分到“扩展表“。 一般是针对那种几百列的大表，也避免查询时，数据量太大造成的“跨页”问题。
水平拆分
针对数据量巨大的单张表（比如订单表），按照某种规则（RANGE,HASH取模等），切分到多张表里面去。 但是这些表还是在同一个库中，所以库级别的数据库操作还是有IO瓶颈。不建议采用。
水平分库分表
将单张表的数据切分到多个服务器上去，每个服务器具有相应的库与表，只是表中数据集合不同。 水平分库分表能够有效的缓解单机和单库的性能瓶颈和压力，突破IO、连接数、硬件资源等的瓶颈。

- exits/in

	- exists和in的使用方式：　　
对B查询涉及id，使用索引，故B表效率高，可用大表 -->外小内大
select * from A where exists (select * from B where A.id=B.id);
"#"对A查询涉及id，使用索引，故A表效率高，可用大表 -->外大内小
select * from A where A.id in (select id from B);
　　1、exists是对外表做loop循环，每次loop循环再对内表（子查询）进行查询，那么因为对内表的查询使用的索引（内表效率高，故可用大表），而外表有多大都需要遍历，不可避免（尽量用小表），故内表大的使用exists，可加快效率；
　　2、in是把外表和内表做hash连接，先查询内表，再把内表结果与外表匹配，对外表使用索引（外表效率高，可用大表），而内表多大都需要查询，不可避免，故外表大的使用in，可加快效率。
　　3、如果用not in ，则是内外表都全表扫描，无索引，效率低，可考虑使用not exists，也可使用A left join B on A.id=B.id where B.id is null 进行优化。

- 主从延迟

	- 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。
打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义

- inoodb四大特性

	- 一：插入缓冲
	- 二：二次写
	- 三：自适应哈希
	- 四：预读

- 百万级别或以上的数据，你是如何删除的？

	- 1.所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟）
2.然后删除其中无用数据（此过程需要不到两分钟）
3.删除完成后重新创建索引(此时数据较少了)创建索引也非常快，约十分钟左右。
4.与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了。

- count(*) 和 count(1)和count(列名)区别  

	- count(*)包括了所有的列，相当于行数，在统计结果的时候，不会忽略列值为NULL  
count(1)包括了忽略所有列，用1代表代码行，在统计结果的时候，不会忽略列值为NULL  
count(列名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是只空字符串或者0，而是表示null）的计数，即某个字段值为NULL时，不统计。

- union/union null

	- UNION 并集，表中的所有数据，并且去除重复数据（工作中主要用到的是这个）；
	- UNION ALL，表中的数据都罗列出来；

- 慢查询分析

	- 1、大多数情况下很正常，偶尔很慢，则有如下原因
	  - (1)、数据库在刷新脏页，例如 redo log 写满了需要同步到磁盘。
	  - (2)、执行的时候，遇到锁，如表锁、行锁。
	- 2、这条 SQL 语句一直执行的很慢，则有如下原因。
	  - (1)、没有用上索引：例如该字段没有索引；由于对字段进行运算、函数操作导致无法用索引。
	  - (2)、数据库选错了索引。

## 项目

### 猫眼交易系统

- 防刷设计

	- 1：前端增加图文验证码和滑块
	- 2：答题策略（直接刷到本地 将结果存到缓存）
	- 3：黑名单（用户id 手机号 ip  收货地址）
	- 待支付时间5分钟
	- 限制生成待支付订单数

- 限流

	- 1：滑动窗口
	- 2：漏桶
		- 流入：以任意速率往桶中放入水滴。
		- 流出：以固定速率从桶中流出水滴。
		- 缺点：因为当流出速度固定，大规模持续突发量，无法多余处理，浪费网络带宽
		- 优点：无法击垮服务
	- 3：令牌桶
		- 流入：以固定速率从桶中流入水滴
		- 流出：按照任意速率从桶中流出水滴
		- 优点：支持大的并发，有效利用网络带宽
	- 4：拒绝服务
	- 5：排队等待
	- 6：降级

- 超卖

	- 乐观锁

		-  乐观锁跟新库存解决超卖问题：悲观锁虽然可以解决超卖问题，但是加锁的时间可能会很长，会长时间的限制其他用户的访问，导致很多请求等待锁，卡死在这里，如果这种请求很多就会耗尽连接，系统出现异常。乐观锁默认不加锁，更失败了就直接返回将狗失败。

```mysql
UPDATE stock SET count = count -1,sale = sale +1 , version = version + 1 where id = #{id} AND version = #{version}
  
UPDATE stock SET count = count -1,sale = sale +1 , version = version + 1 where id = #{id} AND totalStock >=#saleStock# +1
```

​						目前我们使用的是第二种方式这种情形不用版本号，之跟新是做数据安全校验性能更高。
​						注意：乐观锁的更新操作，最好用主键或者唯一索引，这样是行锁，否则更新时会锁表 
​						MySql innoDB给数据加锁是加在索引上来实现的，
​						简单来说我们的语句如果无法命中索引，innoDB就会锁表

- 发现热点数据

	-   1.构建一个异步的系统，他可以手机交易链路上各个环节中的热点key
  -   2.建立一个热点上报和可以按照需求订阅的热点服务的下发规范，主要是通过交易链路上各个系统的访问时间差。
  
- 缓存库存信息

	- 缓存数据库数据一致性

		- 先更新数据库，再更新缓存

			-    缺点： 线程安全问题 多线程情况下导致脏数据。（乐观锁解决？）
	业务场景：写多读少的场景：  数据还没读到，就频繁更改浪费性能。而且如果这个缓存值要经过一些列的计算，那么每次更改都要进行计算会更加浪费性能，这个时候删除更合适。

		- 先删缓存，再进行更新数据库。
			- 更新库存然后删除缓存-读时写缓存：高并发下还是会出现脏数据（读写分离的mysql架构很容易出现）（加过期时间）同时有可能造成击穿。
	
		-  先更新数据库，再删除缓存。  
			- 先读到脏数据然后进行了更新删缓存（加过期时间）同时有可能造成击穿。
	
		- MySQL binlog增量订阅消费+消息队列+增量数据更新到redis
	
	- 直接将库存缓存到本地

### 项目设计

### 负载均衡

总结下负载均衡的常用方案及适用场景

##### 轮询调度
以轮询的方式依次请求调度不同的服务器；实现时，一般为服务器带上权重；这样有两个好处：
1. 针对服务器的性能差异可分配不同的负载；
2. 当需要将某个结点剔除时，只需要将其权重设置为0即可；
优点：实现简单、高效；易水平扩展；
缺点：请求到目的结点的不确定，造成其无法适用于有写的场景（缓存，数据库写）
应用场景：数据库或应用服务层中只有读的场景；

##### 随机方式
请求随机分布到各个结点；在数据足够大的场景能达到一个均衡分布；
优点：实现简单、易水平扩展；
缺点：同Round Robin，无法用于有写的场景；
应用场景：数据库负载均衡，也是只有读的场景；

##### 哈希：
根据key来计算需要落在的结点上，可以保证一个同一个键一定落在相同的服务器上；
优点：相同key一定落在同一个结点上，这样就可用于有写有读的缓存场景；
缺点：在某个结点故障后，会导致哈希键重新分布，造成命中率大幅度下降；
解决：一致性哈希 or 使用keepalived保证任何一个结点的高可用性，故障后会有其它结点顶上来；
应用场景：缓存，有读有写；

##### 一致性哈希：
在服务器一个结点出现故障时，受影响的只有这个结点上的key，最大程度的保证命中率；
如twemproxy中的ketama方案；
生产实现中还可以规划指定子key哈希，从而保证局部相似特征的键能分布在同一个服务器上；
优点：结点故障后命中率下降有限；
应用场景：缓存；

##### 根据键的范围来负载：
根据键的范围来负载，前1亿个键都存放到第一个服务器，1~2亿在第二个结点；
优点：水平扩展容易，存储不够用时，加服务器存放后续新增数据；
缺点：负载不均；数据库的分布不均衡；（数据有冷热区分，一般最近注册的用户更加活跃，这样造成后续的服务器非常繁忙，而前期的结点空闲很多）
适用场景：数据库分片负载均衡；

##### 根据键对服务器结点数取模来负载：
根据键对服务器结点数取模来负载；比如有4台服务器，key取模为0的落在第一个结点，1落在第二个结点上。
优点：数据冷热分布均衡，数据库结点负载均衡分布；
缺点：水平扩展较难；
适用场景：数据库分片负载均衡；

##### 纯动态结点负载均衡：
根据CPU、IO、网络的处理能力来决策接下来的请求如何调度；
优点：充分利用服务器的资源，保证个结点上负载处理均衡；
缺点：实现起来复杂，真实使用较少；

##### 不用主动负载均衡：
使用消息队列转为异步模型，将负载均衡的问题消灭
负载均衡是一种推模型，一直向你发数据，那么，将所有的用户请求发到消息队列中，所有的下游结点谁空闲，谁上来取数据处理；转为拉模型之后，消息了负载的问题；
优点：通过消息队列的缓冲，保护后端系统，请求剧增时不会冲垮后端服务器；
水平扩展容易，加入新结点后，直接取queue即可；
缺点：不具有实时性；
应用场景：不需要实时返回的场景；
比如，12036下订单后，立刻返回提示信息：您的订单进去排队了…等处理完毕后，再异步通知；

## 网路/linux

### 长连接 VS 短连接

- 连接->传输数据->关闭连接
  比如HTTP是无状态的的短链接，浏览器和服务器每进行一次HTTP操作，就建立一次连接，但任务结束就中断连接。
  因为连接后接收了数据就断开了，所以每次数据接受处理不会有联系。 这也是HTTP协议无状态的原因之一。

- 连接->传输数据->保持连接 -> 传输数据-> ...........->直到一方关闭连接，多是客户端关闭连接。
  长连接指建立SOCKET连接后不管是否使用都保持连接，但安全性较差。

- 长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，
  这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都
  不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果
  用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。

  而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网
  站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成
  千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频
  繁操作情况下需用短连好。


### 三次握手/四次挥手

- 三次握手

  - 客户端–发送带有 SYN 标志的数据包–一次握手–服务端

  - 服务端–发送带有 SYN/ACK 标志的数据包–二次握手–客户端

  - 客户端–发送带有带有 ACK 标志的数据包–三次握手–服务端

    三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。
    第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常
    第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常
    第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常
    所以三次握手就能确认双发收发功能都正常，缺一不可。

    socket是双向的  服务端在发向客户端的请求也许想要客户端返回一个ack，这样站在服务端的角度任务socket是通的


- 四次挥手

  - - 客户端-发送一个 FIN，用来关闭客户端到服务器的数据传送
    - 服务器-收到这个 FIN，它发回一 个 ACK，确认序号为收到的序号加1 。和 SYN 一样，一个 FIN 将占用一个序号
    - 服务器-关闭与客户端的连接，发送一个FIN给客户端
    - 客户端-发回 ACK 报文确认，并将确认序号设置为收到序号加1
  - 任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了TCP连接。
      举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。
  - TIME_WAIT:确保最后一个确认报文能够到达（如果B没收到A发送来的确认报文，那么就会重新发送连接释放请求报文，A等待一段时间就是为了处理这种情况的发生）
避免新旧链接混淆。（等待2MSL可以让本连接持续时间内所产生的所有报文都从网络中消失，使得下一个新的连接请求不会出现旧的连接请求报文）

### HTTP/HTTPS

- 1. 端口 ：HTTP的URL由“http://”起始且默认使用端口80，而HTTPS的URL由“https://”起始且默认使用端口443。
  2. 安全性和资源消耗： HTTP协议运行在TCP之上，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份。HTTPS是运行在SSL/TLS之上的HTTP协议，SSL/TLS 运行在TCP之上。所有传输的内容都经过加密，加密采用对称加密，但对称加密的密钥用服务器方的证书进行了非对称加密。所以说，HTTP 安全性没有 HTTPS高，但是 HTTPS 比HTTP耗费更多服务器资源。
  3. 对称加密：密钥只有一个，加密解密为同一个密码，且加解密速度快，典型的对称加密算法有DES、AES等；
  4. 非对称加密：密钥成对出现（且根据公钥无法推知私钥，根据私钥也无法推知公钥），加密解密使用不同密钥（公钥加密需要私钥解密，私钥加密需要公钥解密），相对对称加密速度较慢，典型的非对称加密算法有RSA、DSA等。
### http1.0/http1.1

- HTTP1.0最早在网页中使用是在1996年，那个时候只是使用一些较为简单的网页上和网络请求上，而HTTP1.1则在1999年才开始广泛应用于现在的各大浏览器网络请求中，同时HTTP1.1也是当前使用最为广泛的HTTP协议。 主要区别主要体现在：
1. 长连接 : 在HTTP/1.0中，默认使用的是短连接，也就是说每次请求都要重新建立一次连接。HTTP 是基于TCP/IP协议的,每一次建立或者断开连接都需要三次握手四次挥手的开销，如果每次请求都要这样的话，开销会比较大。因此最好能维持一个长连接，可以用个长连接来发多个请求。HTTP 1.1起，默认使用长连接 ,默认开启Connection： keep-alive。 HTTP/1.1的持续连接有非流水线方式和流水线方式 。流水线方式是客户在收到HTTP的响应报文之前就能接着发送新的请求报文。与之相对应的非流水线方式是客户在收到前一个响应后才能发送下一个请求。
2. 错误状态响应码 :在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。
3. 缓存处理 :在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。
4. 带宽优化及网络连接的使用 :HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。

### 浏览器中输入url地址：显示内容

1. DNS解析

2. TCP连接
3. 发送HTTP请求
4. 服务器处理请求并返回HTTP报文
5. 浏览器解析渲染页面
6. 连接结束

### HTTP是不保存状态的协议,如何保存用户状态?

- session

### session/cookie

- Cookie 和 Session都是用来跟踪浏览器用户身份的会话方式，但是两者的应用场景不太一样。
Cookie 一般用来保存用户信息 比如①我们在 Cookie 中保存已经登录过得用户信息，下次访问网站的时候页面可以自动帮你登录的一些基本信息给填了；②一般的网站都会有保持登录也就是说下次你再访问网站的时候就不需要重新登录了，这是因为用户登录的时候我们可以存放了一个 Token 在 Cookie 中，下次登录的时候只需要根据 Token 值来查找用户即可(为了安全考虑，重新登录一般要将 Token 重写)；③登录一次网站后访问网站其他页面不需要重新登录。Session 的主要作用就是通过服务端记录用户的状态。 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。
- Cookie 数据保存在客户端(浏览器端)，Session 数据保存在服务器端。
- Cookie 存储在客户端中，而Session存储在服务器上，相对来说 Session 安全性更高。如果要在 Cookie 中存储一些敏感信息，不要直接写入 Cookie 中，最好能将 Cookie 信息加密然后使用到的时候再去服务器端解密

### TCP,UDP 协议的区别

- UDP 在传送数据之前不需要先建立连接，远地主机在收到 UDP 报文后，不需要给出任何确认。虽然 UDP 不提供可靠交付，但在某些情况下 UDP 确是一种最有效的工作方式（一般用于即时通信），比如： QQ 语音、 QQ 视频 、直播等等
- TCP 提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。 TCP 不提供广播或多播服务。由于 TCP 要提供可靠的，面向连接的传输服务（TCP的可靠体现在TCP在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源），这一难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。TCP 一般用于文件传输、发送和接收邮件、远程登录等场景。

### TCP 协议如何保证可靠传输

1. 应用数据被分割成 TCP 认为最适合发送的数据块。

2. TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。
3. 校验和： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。
4. TCP 的接收端会丢弃重复的数据。
5. 流量控制： TCP 连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制）
6. 拥塞控制： 当网络拥塞时，减少数据的发送。
7. ARQ协议： 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。
8. 超时重传： 当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段
- 滑动窗口

	- TCP 利用滑动窗口实现流量控制。流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。

- 阻塞控制

	- 为了进行拥塞控制，TCP 发送方要维持一个 拥塞窗口(cwnd) 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。
TCP的拥塞控制采用了四种算法，即 慢开始 、 拥塞避免 、快重传 和 快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理 AQM），以减少网络拥塞的发生。
* 慢开始： 慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd初始值为1，每经过一个传播轮次，cwnd加倍。
* 拥塞避免： 拥塞避免算法的思路是让拥塞窗口cwnd缓慢增大，即每经过一个往返时间RTT就把发送放的cwnd加1.
* 快重传与快恢复： 在 TCP/IP 中，快速重传和恢复（fast retransmit and recovery，FRR）是一种拥塞控制算法，它能快速恢复丢失的数据包。没有 FRR，如果数据包丢失了，TCP 将会使用定时器来要求传输暂停。在暂停的这段时间内，没有新的或复制的数据包被发送。有了 FRR，如果接收机接收到一个不按顺序的数据段，它会立即给发送机发送一个重复确认。如果发送机接收到三个重复确认，它会假定确认件指出的数据段丢失了，并立即重传这些丢失的数据段。有了 FRR，就不会因为重传时要求的暂停被耽误。 　当有单独的数据包丢失时，快速重传和恢复（FRR）能最有效地工作。当有多个数据信息包在某一段很短的时间内丢失时，它则不能很有效地工作。

## mybatis

### \#{}和\${}的区别

-  \${}是 Properties 文件中的变量占位符，它可以用于标签属性值和 sql 内部，属于静态文本替换，比如\${driver}会被静态替换为com.mysql.jdbc.Driver。
\#{}是 sql 的参数占位符，Mybatis 会将 sql 中的#{}替换为?号，在 sql 执行前会使用 PreparedStatement 的参数设置方法，按序给 sql 的?号占位符设置参数值，比如 ps.setInt(0, parameterValue)，\#{item.name} 的取值方式为使用反射从参数对象中获取 item 对象的 name 属性值，相当于 param.getItem().getName()。
- \#{} 解析为一个 JDBC 预编译语句（prepared statement）的参数标记符，一个 #{ } 被解析为一个参数占位符；而${}仅仅为一个纯碎的 string 替换，在动态 SQL 解析阶段将会进行变量替换。
- \#{} 解析之后会将String类型的数据自动加上引号，其他数据类型不会；而${} 解析之后是什么就是什么，他不会当做字符串处理。
- \#{} 很大程度上可以防止SQL注入（SQL注入是发生在编译的过程中，因为恶意注入了某些特殊字符，最后被编译成了恶意的执行操作）；而${} 主要用于SQL拼接的时候，有很大的SQL注入隐患。
- 在某些特殊场合下只能用\${}，不能用\#{}。例如：在使用排序时ORDER BY \${id}，如果使用\#{id}，则会被解析成ORDER BY “id”,这显然是一种错误的写法

### 缓存

- 一级缓存(默认开启 同一个sqlsession)

	- 1、一级缓存的生命周期有多长？
	  - MyBatis在开启一个数据库会话时，会 创建一个新的SqlSession对象，SqlSession对象中会有一个新的Executor对象。Executor对象中持有一个新的PerpetualCache对象；当会话结束时，SqlSession对象及其内部的Executor对象还有PerpetualCache对象也一并释放掉。
	  - 如果SqlSession调用了close()方法，会释放掉一级缓存PerpetualCache对象，一级缓存将不可用。
	  - 如果SqlSession调用了clearCache()，会清空PerpetualCache对象中的数据，但是该对象仍可使用。
	  - SqlSession中执行了任何一个update操作(update()、delete()、insert()) ，都会清空PerpetualCache对象的数据，但是该对象可以继续使用

	- 2、怎么判断某两次查询是完全相同的查询？
	
	  mybatis认为，对于两次查询，如果以下条件都完全一样，那么就认为它们是完全相同的两次查询。
	
	  - 2.1 传入的statementId
	  - 2.2 查询时要求的结果集中的结果范围
	  - 2.3. 这次查询所产生的最终要传递给JDBC java.sql.Preparedstatement的Sql语句字符串（boundSql.getSql() ）
	  - 2.4 传递给java.sql.Statement要设置的参数值

- 二级缓存(application级别的)

	- 映射语句文件中的所有select语句将会被缓存。
映射语句文件中的所欲insert、update和delete语句会刷新缓存。
缓存会使用默认的Least Recently Used（LRU，最近最少使用的）算法来收回。
根据时间表，比如No Flush Interval,（CNFI没有刷新间隔），缓存不会以任何时间顺序来刷新。
缓存会存储列表集合或对象(无论查询方法返回什么)的1024个引用
缓存会被视为是read/write(可读/可写)的缓存，意味着对象检索不是共享的，而且可以安全的被调用者修改，不干扰其他调用者或线程所做的潜在修改。

### 延迟加载

- Mybatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加载，association 指的就是一对一，collection 指的就是一对多查询。在 Mybatis 配置文件中，可以配置是否启用延迟加载 lazyLoadingEnabled=true|false。

  它的原理是，使用CGLIB 创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用 a.getB().getName()，拦截器 invoke()方法发现 a.getB()是 null 值，那么就会单独发送事先保存好的查询关联 B 对象的 sql，把 B 查询上来，然后调用 a.setB(b)，于是 a 的对象 b 属性就有值了，接着完成 a.getB().getName()方法的调用。这就是延迟加载的基本原理。

  当然了，不光是 Mybatis，几乎所有的包括 Hibernate，支持延迟加载的原理都是一样的。

### 分页查询

- Mybatis 使用 RowBounds 对象进行分页，它是针对 ResultSet 结果集执行的内存分页，而非物理分页，可以在 sql 内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。

分页插件的基本原理是使用 Mybatis 提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的 sql，然后重写 sql，根据 dialect 方言，添加对应的物理分页语句和物理分页参数。
- RowBounds是返回所有结果，但在底层并不一定是一次性查询，为了防止内存溢出问题，mybatis配置了fetchSize来控制一次性最多查询的条数，如果需要查询的结果数量超过fetchSize，那么mybatis会吧查询分成多次，比如要返回1000条结果，fetchSize配置了200，那就查询5次，把这五次的结果整合起来再返回

### 接口是如何找到实现的

- Dao 接口，就是人们常说的 Mapper接口，接口的全限名，就是映射文件中的 namespace 的值，接口的方法名，就是映射文件中MappedStatement的 id 值，接口方法内的参数，就是传递给 sql 的参数。Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为 key 值，可唯一定位一个MappedStatement，举例：com.mybatis3.mappers.StudentDao.findStudentById，可以唯一找到 namespace 为com.mybatis3.mappers.StudentDao下面id = findStudentById的MappedStatement。在 Mybatis 中，每一个\<select>、\<insert>、\<update>、\<delete>标签，都会被解析为一个MappedStatement对象。
- Dao 接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略。
- Dao 接口的工作原理是 JDK 动态代理，Mybatis 运行时会使用 JDK 动态代理为 Dao 接口生成代理 proxy 对象，代理对象 proxy 会拦截接口方法，转而执行MappedStatement所代表的 sql，然后将 sql 执行结果返回

### 动态 sql 

- Mybatis 动态 sql 可以让我们在 Xml 映射文件内，以标签的形式编写动态 sql，完成逻辑判断和动态拼接 sql 的功能，Mybatis 提供了 9 种动态 sql 标签 trim|where|set|foreach|if|choose|when|otherwise|bind。

  其执行原理为，使用 OGNL 从 sql 参数对象中计算表达式的值，根据表达式的值动态拼接 sql，以此来完成动态 sql 的功能。

## Spring

### AOP/IOC

- IoC（Inverse of Control:控制反转）是一种设计思想，就是 将原本在程序中手动创建对象的控制权，交由Spring框架来管理。 IoC 在其他语言中也有应用，并非 Spring 特有。 IoC 容器是 Spring 用来实现 IoC 的载体， IoC 容器实际上就是个Map（key，value）,Map 中存放的是各种对象。
将对象之间的相互依赖关系交给 IoC 容器来管理，并由 IoC 容器完成对象的注入。这样可以很大程度上简化应用的开发，把应用从复杂的依赖关系中解放出来。 IoC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。 在实际项目中一个 Service 类可能有几百甚至上千个类作为它的底层，假如我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IoC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。
- AOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。
Spring AOP就是基于动态代理的，如果要代理的对象，实现了某个接口，那么Spring AOP会使用JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候Spring AOP会使用Cglib ，这时候Spring AOP会使用 Cglib 生成一个被代理对象的子类来作为代理

### beanfeatory/applicationContext

- beanFactory 只是对IOC容器最基本行为作了定义，而不关心 Bean 是怎样定义和加载的。
定义了一些getBean、containsBean、isSingleton、isPrototype等基本容器方法
- ApplicationContext是spring中较高级的容器。和BeanFactory类似，它可以加载配置文件中定义的bean，将所有的bean集中在一起，当有请求的时候分配bean。 另外，它增加了企业所需要的功能。

### bean的生命周期

- 实例化 -> 属性赋值 -> 初始化 -> 销毁
- 扩展类

	- BeanPostProcessor（初始化前后）
	- InstantiationAwareBeanPostProcessor（实例化前后）

		- postProcessBeforeInstantiation在doCreateBean之前执行
		- postProcessAfterInstantiation(方法作为属性赋值的前置检查条件，在属性赋值之前执行，能够影响是否进行属性赋值！)

### creatBean()

- 1:resolveBeforeInstantiation(循环调用所有的InstantiationAwareBeanPostProcessor)
- 2:doCreateBean
	- createBeanInstance
	- applyMergedBeanDefinitionPostProcessors
	- populateBean
	- initializeBean

### 循环注入

- 构造器循环依赖

	- 这种循环依赖没有什么解决办法，因为JVM虚拟机在对类进行实例化的时候，需先实例化构造器的参数，而由于循环引用这个参数无法提前实例化，故只能抛出错误。

- setter循环依赖

	- 中singletonObjects里面存放的是初始化之后的单例对象；earlySingletonObjects中存放的是一个已完成实例化未完成初始化的早期单例对象；而singletonFactories中存放的是ObjectFactory对象，此对象的getObject方法返回值即刚完成实例化还未开始初始化的单例对象。所以先后顺序是，单例对象先存在于singletonFactories中，后存在于earlySingletonObjects中，最后初始化完成后放入singletonObjects中。
	
	- 1)检测当前bean是否在singletonObjects中，在则直接返回缓存好的bean；不在则检测是否在singletonFactories中，在，则调用其getObject方法，返回，并从singletonFactories中移除，加入到earlySingletonObjects中。
	
	- 2)正常创建，beforeSingletonCreation:检测当前bean是否在singletonsCurrentlyInCreation，如果存在，抛出异常。表示存在构造器循环依赖。如果不存在，则将当前bean加入。
	
	- 3)bean初始化，分为构造方法初始化、工厂方法初始化和简单初始化。如果是构造方法初始化，那么递归地获取参数bean。其他情况不会递归获取bean。
	
	- 4)addSingletonFactory:如果当前bean不在singletonObjects中，则将当前bean加入到singletonFactories中，并从earlySingletonObjects中移除。
	
	- 5)填充属性，简单初始化的话会递归创建所依赖的bean。
	
	- 6)调用用户初始化方法，比如BeanPostProcesser、InitializingBean、init-method，有可能返回代理后的bean。
	
	  检测循环依赖，如果当前bean在singletonObjects中，则判断当前bean(current bean)与singletonObjects中的bean(cached bean)是否是同一个，如果不是，那么说明当前bean是被代理过的，由于依赖当前bean的bean持有的是对cached bean的引用，这是不被允许的，所以会抛出BeanCurrentlyInCreationException异常。
	
	- 7)afterSingletonCreation:将当前bean从singletonsCurrentlyInCreation中删除
	
	- 8)addSingleton:将当前bean加入到singletonObjects，然后从singletonFactories, earlySingletonObjects中移除，结束


### 设计模式

- 工厂模式
- 单例模式
- 代理模式
- 观察者

	- 观察者模式是一种对象行为型模式。它表示的是一种对象与对象之间具有依赖关系，当一个对象发生改变的时候，这个对象所依赖的对象也会做出反应。Spring 事件驱动模型就是观察者模式很经典的一个应用。Spring 事件驱动模型非常有用，在很多场景都可以解耦我们的代码。比如我们每次添加商品的时候都需要重新更新商品索引，这个时候就可以利用观察者模式来解决这个问题

- 适配器

	- 适配器模式(Adapter Pattern) 将一个接口转换成客户希望的另一个接口，适配器模式使接口不兼容的那些类可以一起工作，其别名为包装器(Wrapper)。

### 事务

- 分类

	- 编程式事务
	- 声明式事务

- 事务的传播行为

	- PROPAGATION_REQUIRED

		- 如果当前存在一个事务，则加入当前事务；如果不存在任何事务，则创建一个新的事务。总之，要至少保证在一个事务中运行。PROPAGATION_REQUIRED通常作为默认的事务传播行为。
	REQUIRED（内层事务newTransaction为false，内层事务回滚时仅设置回滚标记，外层事务进行外层回滚)

	- PROPAGATION_SUPPORTS

		- 如果当前存在一个事务，则加入当前事务；如果当前不存在事务，则直接执行。 对于一些查询方法来说，PROPAGATION_SUPPORTS通常是比较合适的传播行为选择。 如果当前方法直接执行，那么不需要事务的支持；如果当前方法被其他方法调用，而其他方法启动了一个事务的时候，使用PROPAGATION_SUPPORTS可以保证当前方法能够加入当前事务并洞察当前事务对数据资源所做的更新。

	- PROPAGATION_REQUIRES_NEW

		- 不管当前是否存在事务，都会创建新的事务。如果当前存在事务的话，会将当前的事务挂起(suspend)。 如果某个业务对象所做的事情不想影响到外层事务的话，PROPAGATION_REQUIRES_NEW应该是合适的选择，比如，假设当前的业务方法需要向数据库中更新某些日志信息， 但即使这些日志信息更新失败，我们也不想因为该业务方法的事务回滚而影响到外层事务的成功提交，因为这种情况下，当前业务方法的事务成功与否对外层事务来说是无关紧要的。
	REQUIRES_NEW（内外层事务平级，内层事务newTransaction为true，suspend外层事务，抛出异常后内层事务进行内层回滚，resume外层事务，外层事务捕获到内层抛出的异常后进行外层回滚)

	- PROPAGATION_NESTED（重要，内部事务是外部事务的子事务。子事务出现异常会回滚到savepoint，外部事务提交/回滚时子事务也进行提交/回滚)

		- 如果存在当前事务，则在当前事务的一个嵌套事务中执行，否则与PROPAGATION_REQUIRED的行为类似，即创建新的事务，在新创建的事务中执行。 PROPAGATION_NESTED粗看起来好像与PROPAGATION_REQUIRES_NEW的行为类似，实际上二者是有差别的。 PROPAGATION_REQUIRES_NEW创建的新事务与外层事务属于同一个“档次”，即二者的地位是相同的，当新创建的事务运行的时候，外层事务将被暂时挂起(suspend)； 而PROPAGATION_NESTED创建的嵌套事务则不然，它是寄生于当前外层事务的，它的地位比当前外层事务的地位要小一号，当内部嵌套事务运行的时候，外层事务也是处于active状态。是已经存在事务的一个真正的子事务. 嵌套事务开始执行时, 它将取得一个 savepoint. 如果这个嵌套事务失败, 我们将回滚到此 savepoint. 嵌套事务是外部事务的一部分, 只有外部事务结束后它才会被提交，外部事务回滚，它也会被回滚。

### MVC执行流程

- （1）客户端（浏览器）发送请求，直接请求到 DispatcherServlet。
（2）DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。
（3）解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。
（4）HandlerAdapter 会根据 Handler 来调用真正的处理器开处理请求，并处理相应的业务逻辑。
（5）处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。
（6）ViewResolver 会根据逻辑 View 查找实际的 View。
（7）DispaterServlet 把返回的 Model 传给 View（视图渲染）。
（8）把 View 返回给请求者（浏览器）

### springbean的加载过程

![image-20210730113408477](https://raw.githubusercontent.com/lxcsjk/oss/master/uPic/image-20210730113408477.png)

## kafka

### 为什么那么快

- **partition 并行处理**
- **顺序写磁盘，充分利用磁盘特性**
- **利用了现代操作系统分页存储 Page Cache 来利用内存提高 I/O 效率**
- **采用了零拷贝技术 Producer 生产的数据持久化到 broker，采用 mmap 文件映射，实现顺序的快速写入 Customer 从 broker 读取数据，采用 sendfile，将磁盘文件读到 OS 内核缓冲区后，转到 NIO buffer 进行网络发送，减少 CPU 消耗**

### 优点：

以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。
高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。
支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。
同时支持离线数据处理和实时数据处理。

### 解耦：
在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。消息系统在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口。这允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。
冗余：
有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的"插入-获取-删除"范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。
扩展性：
因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。
灵活性 & 峰值处理能力：
在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。
可恢复性：
系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。
顺序保证：
在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。Kafka保证一个Partition内的消息的有序性。
缓冲：
在任何重要的系统中，都会有需要不同的处理时间的元素。例如，加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行———写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。
异步通信：
很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。


### 常见消息队列对比

- kafka:高吞吐 低延迟  高可用 集群容错性非常好 producer提供了缓存，压缩功能，可提高性能和效率。 提供顺序消费的能力   

  缺点：消费者受分区数限制 单机topic比较多时性能下降明显   不支持事务

- RabbitMQ:高吞吐和高可用不如其他两个  消息吞吐量有限  堆积时 性能明显下降 对负载均衡支持不好。

- RocketMQ:高吞吐 低延迟 高可用，支持多种消费模式 提供消费顺序想  缺点：消费堆积和吞吐量不如kafka  不支持主从切换    客户端只支持java

### kafka

- Kafka的最小存储单元是分区，一个topic包含多个分区，在创建主题时，这些分区被分配在多个服务器上，通常一个broker一台服务器。
- 分区首领会均匀地分布在不同的服务器上，分区副本也会均匀的分布在不同的服务器上，确保负载均衡和高可用性，当新的broker加入集群的时候，部分副本会被移动到新的broker上。
- 根据配置文件中的目录清单，Kafka会把新的分区分配给目录清单里分区数最少的目录。
- 默认情况下，分区器使用轮询算法把消息均衡地分布在同一个主题的不同分区中，对于发送时指定了key的情况，会根据key的hashcode取模后的值存到对应的分区中。

### 生产过程分析

- producer采用推（push）模式将消息发布到broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）。
由于生产者生产的消息会不断追加到patition，为了防止log过大导致数据定位效率低 kafka采用分片和索引的机制（默认偏移量是1G，将每个patition分为多个segment，每个segement对应两个文件-“，Index”/"。log",文件的命名规则：topic名称+分区号）
index
- partition（分区）

	- 消息发送时都被发送到一个topic，其本质就是一个目录，而topic是由一些Partition Logs(分区日志)组成，其组织结构如下图所示：
我们可以看到，每个Partition中的消息都是有序的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个唯一的offset值。
1）分区的原因
（1）方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；
（2）可以提高并发，因为可以以Partition为单位读写了。
2）分区的原则
（1）指定了patition，则直接使用；
（2）未指定patition但指定key，通过对key的value进行hash出一个patition；
（3）patition和key都未指定，使用轮询选出一个patition。

### 数据可靠性

- topic接受消息可靠性

	- 为保证producer发送的数据能够可靠地发送到指定的topic上，topic的每个partition在接收到消息后，都会向producer发送ack，如果producer收到ack后，就会进行下一轮的发送，否则重发消息                                       何时ack：确保follower与leader同步完成，leader再发送ack，这样才能保证leader挂掉之后，能在follwer中选举出新的leader  而kafka的follwe并不是完全同步 也不是完全异步，而是一种ISR机制。
	
	- kafka的ISR机制
	  leader会维护一个与其基本保持同步的Replica列表，该列表称为ISR(in-sync Replica)，每个Partition都会有一个ISR，而且是由leader动态维护。
	
	  如果一个Follower超过一定时间未发起数据复制请求，则leader将其从ISR中移除。
	
	  如果一个Follower的消息偏移量追赶上Leader的同步消息偏移量，则leader将其放入ISR中。
	
	  Leader收到ISR里的全部Follower的ACK以后再向发送者回复ACK。
	
	  ack参数：
	
	   0：producer不等待broker的ack  
	
	   1：partition的leader落盘成功后返回ack 
	
	   -1：leader 和 follwer全部落盘成功后才返回ack)

- 故障处理

	- LEO：指的是每个副本最大的offset；
HW：指的是消费者能见到的最大的offset，ISR队列中最小的LEO。
（1）follower故障
follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了。
（2）leader故障
leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。
注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。

- 没有ack 导致消息重发

	- 精准一次性=重复数据（至少一次）+ 幂等性
	
	- 幂等性：无论调用者发送多少次消息,服务端都只处理一次。
	
	- kafka集群会给开启幂等性的生产者在初始化的时候会分配一个PID。生产者发往同一个分区（Partition）的每一个消息会附带上一个序列号（SeqNumber）。Broker端会对<PID,Partition,SeqNumber>做缓存,当具有相同序列号的消息提交时，Broker只会持久化一条消息。
	
	  但生产者的PID在重启以后会变化，同时不同的分区也具有不同序列号，所以幂等性无法保证跨分区跨会话的精准一次性。

### kafka消费者

- 消费模式

	- Push模式：很难适应消费速率不同的消费者，因为消费发送速率是由broker决定的。它的目标是尽可能以最快的速度传递消息，这样很容易造成consumer来不及消费消息。
	- Pull模式: 如果kafka没有数据,消费者会陷入循环中，一直返回空数据。可以传入一个timeout参数，如果没有数据可供消费consumer就等待一段时间之后再返回。
	- Push模式和Pull模式都有各自的优势和缺点，相较之下，Kafka选择了Pull模式，以提高整个集群的消费能力

- 消费分区策略（需保证两个原则：所有的分区的消息都有消费者进行消费，不可能出现同一个消费者组的多个个消费者负责同一个分区。
	- RoundRobin轮询：将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序，然后通过轮询方式逐个将分区以此分配给每个消费者
	- Range范围：将消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。

- Rebalance

	- Rebalance本身是Kafka集群的一个保护设定，用于剔除掉无法消费或者过慢的消费者，然后由于我们的数据量较大，同时后续消费后的数据写入需要走网络IO，很有可能存在依赖的第三方服务存在慢的情况而导致我们超时。
	- Rebalance 的触发条件有3个。
	  - 组成员个数发生变化。例如有新的 consumer 实例加入该消费组或者离开组。
	  - 订阅的 Topic 个数发生变化。
	  - 订阅 Topic 的分区数发生变化。
	- 在 Rebalance 的过程中 consumer group 下的所有消费者实例都会停止工作，等待 Rebalance 过程完成。后两个我们大可以人为的避免，发生rebalance最常见的原因是消费组成员的变化。我们可以设置 session.timeout.ms 和 heartbeat.interval.ms 的值，保证心跳机制第二类非必要 Rebalance 是 Consumer 消费时间过长导致的。此时，max.poll.interval.ms 参数值的设置显得尤为关键。如果要避免非预期的 Rebalance，你最好将该参数值设置得大一点，比你的下游最大处理时间稍长一点。

### kafka高效读写数据

- 顺序写磁盘
Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s，而随机写只有100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。
- 零复制技术      
- 批量发送   
- 日志保留和压缩策略

### 消息丢失/重复/挤压解决方案

- 丢失

	- 唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边自动提交了 offset，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。
	
	  这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要关闭自动提交 offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是可能会有重复消费，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。
	
	  生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了
	
	  这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。
	
	  生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。
	
	  所以此时一般是要求起码设置如下 4 个参数：
	
	  - 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。
	
	  - 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。
	
	  - 在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。
	
	  - 在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。
	
	    我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。

- 重复

	- kafka做 Exectly one （PID  patitionid  做唯一键）
	- 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。
比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。
比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。
比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。

- 积压

	- 先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。
	新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。
	然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 queue。
	接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。
	等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的 consumer 机器来消费消息。
	- 过期失效：手动查出来 补到kafka里

### 其他问题

- 几种数据保留的策略

	- 按照过期时间保留和按照存储的消息大小保留。
